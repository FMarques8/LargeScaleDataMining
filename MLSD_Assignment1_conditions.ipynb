{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa7saSnvRSl3"
      },
      "source": [
        "## Large Scale Data Mining assignment 1\n",
        "### Frequent Itemsets and Association Rules\n",
        "### 1.1. Apriori Algorithm implementation\n",
        "\n",
        "Francisco Marques 97639 - Mestrado em Ciência de Dados\n",
        "franciscocmarques@ua.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl8ESe6rMaw-",
        "outputId": "400d1e03-a989-4d30-df10-f095ffc69fed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.3.2)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyspark) (0.10.9.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCZJv8krMdR9",
        "outputId": "29b0d164-673f-424b-b2f3-a9440825d93e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.3.2\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "sc = SparkContext(appName=\"Assignment1 - Conditions\")\n",
        "print(sc.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "irX1vewRO2TY",
        "outputId": "d6638710-d6eb-4cbe-f9c7-4dbc1ad5fc46"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://192.168.40.117:4041\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Assignment1 - Conditions</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x245cd897310>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark = SparkSession.builder.appName(\"Assignment1 - Conditions\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5XoA0Q4Q6Pq"
      },
      "source": [
        "Load file as Spark DataFrame. With printSchema() we can see the different types in this file. Proved by printing the dataframe, which shows the first few rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8NM73sVGvvO",
        "outputId": "f09fe443-49c7-4035-ac97-e76d3426d822"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame[START: string, STOP: string, PATIENT: string, ENCOUNTER: string, CODE: string, DESCRIPTION: string]\n",
            "root\n",
            " |-- START: string (nullable = true)\n",
            " |-- STOP: string (nullable = true)\n",
            " |-- PATIENT: string (nullable = true)\n",
            " |-- ENCOUNTER: string (nullable = true)\n",
            " |-- CODE: string (nullable = true)\n",
            " |-- DESCRIPTION: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "file = spark.read.option('header','true').csv(\"data/conditions.csv.gz\")\n",
        "file.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmI46tlJVgCB"
      },
      "source": [
        "Here we save the codes and respective descriptions for when we save the results to a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2KVacEJ8OQF_"
      },
      "outputs": [],
      "source": [
        "code_to_desc = dict(file.select('CODE', 'DESCRIPTION').distinct().collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qifbsfVSFYB"
      },
      "source": [
        "First we select the columns with patient ID and condition code, and remove any duplicates, this is, we assume for this that people can only have a given condition one time, even if they're diagnosed multiple times. After this, we order the conditions' codes. This will help when we're searching for the frequent itemsets.\n",
        "\n",
        "Then we create the first frequent itemsets RDD, for $k=1$, although this is simply to help us in finding for $k=2,3$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gy7Wz12RGxNy",
        "outputId": "f8b3639d-96cc-46de-ad46-cbcfce82a96b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 most frequent conditions\n",
            "[(444814009, 751940), (195662009, 524692), (10509002, 461495), (162864005, 365567), (271737000, 355372), (15777000, 354315), (59621000, 305134), (40055000, 250239), (72892002, 205390), (19169002, 201894)]\n"
          ]
        }
      ],
      "source": [
        "from operator import add # equivalent to lambda x, y : x + y\n",
        "\n",
        "support_threshold = 1000 # assignment support threshold value\n",
        "\n",
        "# cleaned file RDD\n",
        "patient_rdd = (file.select('PATIENT', 'CODE') \n",
        "                .distinct()\n",
        "                .rdd.map(lambda row: (row[0], int(row[1]))) # convert DataFrame to RDD with records saved as tuples\n",
        "                .groupByKey() # group values (codes) by patient\n",
        "                .mapValues(tuple) # turn groupByKey iterable to tuple\n",
        "                .map(lambda item: (item[0], sorted(item[1], reverse = True))) # sort codes by decreasing order\n",
        "                )\n",
        "\n",
        "# k = 1 frequent k-sets RDD\n",
        "frequent_items = (patient_rdd.flatMap(lambda item: item[1]) # apply flatmap to the codes\n",
        "                .map(lambda item: (item, 1)) # convert each code to (code, 1)\n",
        "                .reduceByKey(add) # count codes\n",
        "                .filter(lambda x: x[1] >= support_threshold) # remove codes that are not frequent, accordingly to threshold\n",
        "                .sortBy(lambda x: -x[1]) # sort from most frequent to least frequent\n",
        "                )\n",
        "\n",
        "print('10 most frequent conditions')\n",
        "print(frequent_items.take(10))\n",
        "\n",
        "# converts (code, count) to code,  and transforms collected list to set for faster lookup\n",
        "frequent_items_set = set(frequent_items.map(lambda item: item[0]).collect()) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWAaxyPHRJhS"
      },
      "source": [
        "Compare the number of records before and after grouping them by patient. For this we assumed that each patient can only have the same disease once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vm-iJOe0RJl_",
        "outputId": "a686adef-7ae2-43dd-f742-d176880cb75d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of records in original dataframe: 8493211\n",
            "Number of records after removing duplicate diagnosis: 1157578\n"
          ]
        }
      ],
      "source": [
        "print(f'Number of records in original dataframe: {file.rdd.count()}')\n",
        "print(f'Number of records after removing duplicate diagnosis: {patient_rdd.count()}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E-pTlLrobz1-"
      },
      "source": [
        "### $k=2$ frequent k-itemsets\n",
        "Now we find the 10 most frequent pairs of conditions, i.e., pairs. We will make use of itertools' combinations functions to get every possible combination. This algorithm discards candidate pairs that aren't made up of only frequent items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmN3WlyoLQwt",
        "outputId": "17e1231d-4ed5-4b22-efa6-d6891ae9dba7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 most frequent pairs (k = 2 sets)\n",
            "[((444814009, 195662009), 343651), ((444814009, 10509002), 302516), ((271737000, 15777000), 289176), ((444814009, 162864005), 243812), ((444814009, 271737000), 236847), ((444814009, 15777000), 236320), ((195662009, 10509002), 211065), ((444814009, 59621000), 203450), ((195662009, 162864005), 167438), ((444814009, 40055000), 165530)]\n"
          ]
        }
      ],
      "source": [
        "from itertools import combinations\n",
        "\n",
        "# create combinations of the codes of each patient and keeps pairs that are in the frequent set of k = 1\n",
        "# as these combinations are of 2 elements, patients with only 1 code are not considered.\n",
        "frequent_pairs = (patient_rdd.flatMap(lambda item: [pair for pair in combinations(item[1] , 2) \n",
        "                if all(value in frequent_items_set for value in pair)]) # verifies if both elements in the pair are frequent\n",
        "                .map(lambda item: (item, 1))\n",
        "                .reduceByKey(add) # counts items\n",
        "                .filter(lambda item: item[1] >= support_threshold) # removes non-frequent pairs\n",
        "                .sortBy(lambda item: -item[1]) # sorts from most frequent to least\n",
        "                )\n",
        "\n",
        "print(\"10 most frequent pairs (k = 2 sets)\")\n",
        "most_frequent_pairs = frequent_pairs.take(10)\n",
        "print(most_frequent_pairs)\n",
        "\n",
        "# same as above\n",
        "frequent_pairs_set = set(frequent_pairs.map(lambda item: item[0]).collect())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "p4eV1NIecIq_"
      },
      "source": [
        "### $k=3$\n",
        "And the same for the triplets. Now we make use of the frequent pairs set to verify if the triplets' subsets are frequent and keep triplets that verify this condition for every combination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6H7S3QhxbwzX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 most frequent triplets (k = 3 sets)\n",
            "[((444814009, 271737000, 15777000), 192819), ((444814009, 195662009, 10509002), 139174), ((271737000, 195662009, 15777000), 132583), ((271737000, 15777000, 10509002), 115510), ((444814009, 195662009, 162864005), 111860), ((444814009, 271737000, 195662009), 108560), ((444814009, 195662009, 15777000), 108083), ((271737000, 59621000, 15777000), 99818), ((444814009, 162864005, 10509002), 97384), ((444814009, 271737000, 10509002), 94793)]\n"
          ]
        }
      ],
      "source": [
        "frequent_triplets = (patient_rdd.flatMap(lambda item: [triplet for triplet in combinations(item[1], 3) \n",
        "                    if all(subset in frequent_pairs_set for subset in combinations(triplet, 2))])\n",
        "                    # verifies if every subset of length 2 of the triplet is frequent;\n",
        "                    # discards triplets with at least 1 subset that doesn't verify this\n",
        "                    .map(lambda item: (item, 1)) # same as above\n",
        "                    .reduceByKey(add)\n",
        "                    .filter(lambda item: item[1] >= support_threshold)\n",
        "                    .sortBy(lambda item: -item[1])\n",
        "                    )\n",
        "\n",
        "print(\"10 most frequent triplets (k = 3 sets)\")\n",
        "most_frequent_triplets = frequent_triplets.take(10)\n",
        "print(most_frequent_triplets)\n",
        "\n",
        "frequent_triplet_set = set(frequent_triplets.map(lambda item: item[0]).collect())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5FjY7aGGYEWt"
      },
      "source": [
        "#### 1.2 Association Rules generation\n",
        "Now we want to get every and any association rule $\\{X\\}\\rightarrow\\{Y\\}$ and $\\{X,Y\\}\\rightarrow\\{Z\\}$ with a minimum standardized lift of $0.2$. This means that we want to obtain the association rules using the frequent pairs, and then using the frequent triplets. \n",
        "\n",
        "First we count the number of frequent items, *total_singles*; pairs, *total_pairs*; and triplets, *total_triplets*.\n",
        "\n",
        "We can see that we have many more triplets than items or pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cTmUbSBjbw2R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total single items: 7290639\n",
            "Total pairs: 24939331\n",
            "Total triplets: 52773489\n"
          ]
        }
      ],
      "source": [
        "# Simple implementation of counting the total items, pairs and triplets\n",
        "\n",
        "total_singles = sum(frequent_items.map(lambda item: item[1]).collect())\n",
        "\n",
        "total_pairs = sum(frequent_pairs.map(lambda item: item[1]).collect())\n",
        "\n",
        "total_triplets = sum(frequent_triplets.map(lambda item: item[1]).collect())\n",
        "\n",
        "print(f'Total single items: {total_singles}')\n",
        "print(f'Total pairs: {total_pairs}')\n",
        "print(f'Total triplets: {total_triplets}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set a standardized lift threshold and define the function to compute it, accordingly to the class slides: $$\\Large \\text{Std Lift}(I→j)=\\frac{\\text{Lift}(I→j)-\\frac{\\max{\\{P(I)+P(j)-1, 1/n\\}}}{P(I)P(j)}}{\\frac{1}{\\max{\\{P(I),P(j)\\}}}- \\frac{\\max{\\{P(I)+P(j)-1, 1/n\\}}}{P(I)P(j)}}$$ where $n$ is the number of baskets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "c7Ck1Beabw44"
      },
      "outputs": [],
      "source": [
        "lift_threshold = 0.2 # as per the assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JkAqUYzpQlj-"
      },
      "outputs": [],
      "source": [
        "def std_lift(lift, probX, probY, baskets):\n",
        "  numerator = lift - (max([probX + probY - 1, 1/baskets])/ (probX * probY))\n",
        "  denominator = (1/max([probX, probY])) - (max([probX + probY - 1, 1/baskets])/(probX * probY))\n",
        "  return (numerator / denominator)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "r3TDjpouQquX"
      },
      "source": [
        "With this we incrementally compute the different metrics for the association rules: the confidence, then the interest, the lift, and lastly the standardized lift. This is not the same order as in the results files.\n",
        "\n",
        "For some metrics we also need the probability of $X$, $P(X)$, and the probability of $X$ and $Y$, $P(Y\\cap X)$. For this we simply divide each item or pair counts by their respective total counts, as shown below. These metrics are all computed accordingly to the equations provided in class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2If7viDwbw7Z"
      },
      "outputs": [],
      "source": [
        "# Compute probabilities of each item and pair being 'drawn'\n",
        "item_probability = frequent_items.map(lambda item: (item[0], item[1] / total_singles)).collect()\n",
        "pair_probability = frequent_pairs.map(lambda item: (item[0], item[1] / total_pairs)).collect()\n",
        "\n",
        "# Collect to lift to iterate over\n",
        "frequent_items_list = frequent_items.collect()\n",
        "\n",
        "# Confidence\n",
        "XY_confidence = frequent_pairs.flatMap(lambda pair: [(X, tuple(set(pair[0]) - set((X,)))[0], pair[1] / supportX) \n",
        "                                                     for X, supportX in frequent_items_list \n",
        "                                                     if X in pair[0]]\n",
        "                                       )\n",
        "\n",
        "# Interest\n",
        "XY_interest = XY_confidence.flatMap(lambda pair: [((pair[0]), pair[1], pair[2] - probY) \n",
        "                                                  for Y, probY in item_probability \n",
        "                                                  if Y == pair[1]]\n",
        "                                    )\n",
        "\n",
        "# Lift\n",
        "XY_lift = XY_confidence.flatMap(lambda pair: [((pair[0]), pair[1], pair[2] / probY) \n",
        "                                              for Y, probY in item_probability \n",
        "                                              if Y == pair[1]])\n",
        "\n",
        "# Std Lift\n",
        "XY_stdLift = XY_lift.flatMap(lambda pair: [((pair[0]), pair[1], std_lift(pair[2], probX, probY, total_pairs)) \n",
        "                                           for X, probX in item_probability for Y, probY in item_probability \n",
        "                                           if X == pair[0] and Y == pair[1] and X != Y]\n",
        "                             )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can join each RDD as a DataFrame on the same $X$ and $Y$ as shown below. This will enable us to easily sort and filter using the *std_lift* column. As we have a large data set, we decided on leaving every decimal in the results, but we could easily reduce the size of each number by using the function *round* or with string formatting as it was done to align the strings when saving to text files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PdUmNKuNlFiw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+---------+--------------------+-------------------+------------------+-------------------+\n",
            "|        X|        Y|          confidence|           interest|              lift|           std_lift|\n",
            "+---------+---------+--------------------+-------------------+------------------+-------------------+\n",
            "|271737000| 15777000|    0.81372758686672| 0.7651289660873891|16.743841158817425| 0.8161549645249677|\n",
            "| 15777000| 44465007| 0.08957566007648561|0.07337159346449179| 5.527974071038692| 0.2686501058155572|\n",
            "|239873007| 15777000|  0.5051967213114754|0.45659810053214456|10.395289273853983| 0.5051943500163413|\n",
            "|444814009| 82423001|0.048610261457031144|0.04106990731249045| 6.446681485408158| 0.6648943507275152|\n",
            "| 44465007|195662009| 0.45988589615534375| 0.3879179932047245| 6.390152794515829|0.45988455962823005|\n",
            "|195662009| 15777000| 0.30918519817340456| 0.2605865773940737|  6.36201590117763| 0.4578604956427334|\n",
            "|271737000| 68496003|  0.1280967549497428|0.11635745473202525|10.911787974926542| 0.5318782426260087|\n",
            "|195662009| 40055000| 0.21816608600855358| 0.1838427571480792| 6.356204169339372|0.45744205096459417|\n",
            "|162864005| 15777000|   0.282768958904934|0.23417033812560312| 5.818456457620222|  0.291748283175264|\n",
            "|195662009| 43878008| 0.13219946177948205|0.11120418276484424| 6.296628003243644|0.45315406867361685|\n",
            "| 44465007| 10509002|  0.4061521271733058| 0.3428524630423565|6.4163393716132635|0.40615065768051195|\n",
            "| 40055000|195662009| 0.45744268479333755| 0.3854747818427183| 6.356204169339372|0.45744205096459417|\n",
            "|156073000| 72892002|                 1.0| 0.9718282581266197| 35.49656263693461|                1.0|\n",
            "| 68496003| 10509002| 0.39605313891128324|0.33275347478033396| 6.256796846377576|0.39605107603490314|\n",
            "| 59621000| 44054006|  0.1510385601080181|0.14043509997235648|14.244271037530867| 0.5961617719705705|\n",
            "|195662009| 68496003| 0.07429501498021697|0.06255571476249942|6.3287430768732875|0.45546450761475454|\n",
            "| 40055000| 75498004| 0.15192675801933353|0.14274361234444824|16.544086677709306| 0.5678462407781285|\n",
            "| 64859006|444814009|  0.6677456103867923| 0.5646078744489684| 6.474309371977489| 0.6677438539972862|\n",
            "|195662009|162864005|  0.3191167389630488|0.26897476923995595| 6.364264122956458|0.45802230934673793|\n",
            "|429007001|410429000|                 1.0| 0.9940804914356616| 168.9329425122228|                1.0|\n",
            "+---------+---------+--------------------+-------------------+------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Convert RDDs to DataFrames and join them on columns 'X' and 'Y'\n",
        "XY_results = (XY_confidence.toDF(['X', 'Y', 'confidence'])\n",
        "            .join(XY_interest.toDF(['X', 'Y', 'interest']), on = ['X', 'Y'])\n",
        "            .join(XY_lift.toDF(['X', 'Y', 'lift']), on = ['X', 'Y'])\n",
        "            .join(XY_stdLift.toDF(['X', 'Y', 'std_lift']), on = ['X', 'Y'])\n",
        "            )\n",
        "\n",
        "XY_results.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we remove any rule below the standardized lift threshold, and then order them from highest standardized lift to lowest. We can notice that the first values have a value slightly over $1$, but it can be assumed that this is caused by the very large data set, and that it should be considered as simply $1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "d4XNPRtxMFEG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+--------------+--------------------+--------------------+------------------+------------------+\n",
            "|             X|             Y|          confidence|            interest|              lift|          std_lift|\n",
            "+--------------+--------------+--------------------+--------------------+------------------+------------------+\n",
            "|     302870006| 1551000119108|  0.1540293715127908| 0.15242388809768823| 95.93955942730814|1.0000000000000002|\n",
            "|     302870006|      80394007| 0.46354879460995896|    0.45871711936174| 95.93955942730814|1.0000000000000002|\n",
            "|     127013003|90781000119102|  0.1404547949955214| 0.13978804959775862|210.65731457135428|1.0000000000000002|\n",
            "|     302870006| 1501000119109|0.039938414569954735|  0.0395221273281917| 95.93955942730814|1.0000000000000002|\n",
            "|     254637007|     424132000|   0.999686299113795|  0.9979378926983903| 571.7699788251903|1.0000000000000002|\n",
            "|     302870006|97331000119101| 0.05646647015475313|0.055877907204372516| 95.93955942730814|1.0000000000000002|\n",
            "|     302870006|     237602007|  0.9789846299610485|  0.9687804489557896| 95.93955942730814|1.0000000000000002|\n",
            "|     127013003|     431856006| 0.13620734491028344| 0.13556076235421394|210.65731457135428|1.0000000000000002|\n",
            "|     422034002| 1551000119108|  0.5722037543996872|  0.5705982709845846|356.40589558075874|1.0000000000000002|\n",
            "|     126906006|     314994000| 0.16584927816536577| 0.16519062533671797|251.80075291842235|1.0000000000000002|\n",
            "|90781000119102|     431856006|  0.9697593087842008|  0.9691127262281313|1499.8228759514504|1.0000000000000002|\n",
            "|     162573006|     424132000|  0.8488379836185657|  0.8470895772031609|485.49237530798433|1.0000000000000002|\n",
            "|     703151001|     128613002|                 1.0|  0.9941441346910744|170.76895509802543|               1.0|\n",
            "|     156073000|      72892002|                 1.0|  0.9718282581266197| 35.49656263693461|               1.0|\n",
            "|     237602007|     302870006|                 1.0|  0.9895767709798826| 95.93955942730813|               1.0|\n",
            "|     431855005|      44054006|                 1.0|  0.9893965398643384| 94.30883760639537|               1.0|\n",
            "|     410429000|     429007001|                 1.0|  0.9940804914356616| 168.9329425122228|               1.0|\n",
            "|      72892002|     156073000|   0.175193534251911| 0.17025802722708094| 35.49656263693461|               1.0|\n",
            "|      80394007|      44054006|                 1.0|  0.9893965398643384| 94.30883760639537|               1.0|\n",
            "|      19169002|     156073000| 0.17822718852467137|  0.1732916814998413|36.111221730214865|               1.0|\n",
            "+--------------+--------------+--------------------+--------------------+------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Select std_lift column and filter using those values\n",
        "XY_results_filtered = XY_results.filter(XY_results.std_lift >= lift_threshold)\n",
        "\n",
        "# Sort in descending order\n",
        "XY_results_filtered = XY_results_filtered.sort(XY_results_filtered.std_lift.desc())\n",
        "\n",
        "XY_rules = XY_results_filtered.collect()\n",
        "\n",
        "XY_results_filtered.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results are now sorted and organized and we can save them to a text file. We will save using the codes and also the condition description. This is where the code_to_desc dictionary comes in handy. A lot of string formatting was tested in order to obtain the most readable printing possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qBOsDnn2NFfd"
      },
      "outputs": [],
      "source": [
        "with open('XY_rules_with_codes.txt', 'w') as f:\n",
        "  f.write(f\"{'X':<15} {'->':<5} {'Y':<15}\\t{'Std Lift':<20}\\t{'Lift':<20}\\t{'Confidence':<20}\\t{'Interest':<20}\\n\")\n",
        "  for rule in XY_rules:\n",
        "    f.write(f\"{rule['X']:<15} {'->':<5} {rule['Y']:<15}\\t{rule['std_lift']:<20}\\t{rule['lift']:<20}\\t{rule['confidence']:<20}\\t{rule['interest']:<20}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "w-V-AKYBQ3SO"
      },
      "outputs": [],
      "source": [
        "with open('XY_rules_with_description.txt', 'w') as f:\n",
        "  f.write(f\"{'X':<80} {'->':<5} {'Y':<80}\\t{'Std Lift':<20}\\t{'Lift':<20}\\t{'Confidence':<20}\\t{'Interest':<20}\\n\")\n",
        "  for rule in XY_rules:\n",
        "    f.write(f\"{code_to_desc[str(rule['X'])]:<80} {'->':<5} {(code_to_desc[str(rule['Y'])]):<80}\\t{rule['std_lift']:<20}\\t{rule['lift']:<20}\\t{rule['confidence']:<20}\\t{rule['interest']:<20}\\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IKDsNBsxQo75"
      },
      "source": [
        "The association rules of the form $\\{X\\}\\rightarrow\\{Y\\}$ were obtained, so we can now find rules of the form $\\{X,Y\\}\\rightarrow\\{Z\\}$. Here we do the same as previously but using both frequent items and frequent pairs. We need the frequent items for the probability of our $Z$ or $j$ in the class equations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ujGJX3XxeSX0"
      },
      "outputs": [],
      "source": [
        "frequent_pairs_list = frequent_pairs.collect()\n",
        "\n",
        "# Confidence\n",
        "XYZ_confidence = frequent_triplets.flatMap(lambda item: [((X, Y), tuple(set(item[0]) - set((X, Y)))[0], item[1] / supportPair) \n",
        "                                                         for (X, Y), supportPair in frequent_pairs_list \n",
        "                                                         if X in item[0] and Y in item[0]]\n",
        "                                           )\n",
        "\n",
        "# Interest\n",
        "XYZ_interest = XYZ_confidence.flatMap(lambda item: [(item[0], item[1], item[2] - probZ) \n",
        "                                                    for Z, probZ in item_probability \n",
        "                                                    if Z == item[1]]\n",
        "                                      )\n",
        "\n",
        "# Lift\n",
        "XYZ_lift = XYZ_confidence.flatMap(lambda item: [(item[0], item[1], item[2] / probZ) \n",
        "                                                for Z, probZ in item_probability \n",
        "                                                if Z == item[1]]\n",
        "                                  )\n",
        "\n",
        "# Std Lift\n",
        "XYZ_stdLift = XYZ_lift.flatMap(lambda item: [(item[0], item[1], std_lift(item[2], probPair, probZ, total_triplets)) \n",
        "                                             for (X, Y), probPair in pair_probability for Z, probZ in item_probability \n",
        "                                             if X in item[0] and Y in item[0] and Z == item[1] and X != Z and Y != Z]\n",
        "                               )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9fReQnlFNWvZ"
      },
      "source": [
        "Group RDDs into a single DataFrame and print first rows. Then filter and sort using the standardized lift of $0.2$ again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---------+-------------------+-------------------+------------------+-------------------+\n",
            "|              (X, Y)|        Z|         confidence|           interest|              lift|           std_lift|\n",
            "+--------------------+---------+-------------------+-------------------+------------------+-------------------+\n",
            "|{444814009, 19566...| 10509002|  0.404986454280651| 0.3416867901497017| 6.397924220306247|0.40498563604418664|\n",
            "|{271737000, 15777...| 10509002|0.39944532049686005|0.33614565636591076| 6.310386097318297| 0.3994443390653199|\n",
            "|{444814009, 16286...| 15777000|0.28300903975194003|0.23441041897260917|  5.82339653293833| 0.2830076500283272|\n",
            "|{271737000, 10509...|195662009|0.45665622690642926|   0.38468832395581| 6.345276271559053|0.45665441974520743|\n",
            "|{302870006, 23760...| 40055000| 0.2966597217554943|   0.26233639289502| 8.643092951777122| 0.2966552539578859|\n",
            "|{162864005, 68496...|195662009| 0.4528352593388874|0.38086735638826813|  6.29218360926259|0.45282946837607946|\n",
            "|{368581000119106,...|237602007| 0.9943127962085309|  0.984108615203272| 97.44170509089277| 0.9943126463512512|\n",
            "|{410429000, 27173...|429007001|                1.0| 0.9940804914356616| 168.9329425122228|                1.0|\n",
            "|{195662009, 55680...|444814009| 0.6627192263563351| 0.5595814904185112|6.4255746970813155| 0.6627126949309349|\n",
            "|{124171000119105,...| 10509002|0.40009002091662477| 0.3367903567856755| 6.320570992113805| 0.4000825146320407|\n",
            "|{271737000, 16286...|302870006|0.14785198039815026|0.13742875137803282|14.184853859853535|0.14784800970130874|\n",
            "|{49436004, 15777000}|444814009| 0.6677107571147306| 0.5645730211769068| 6.473971442588748| 0.6677037303438154|\n",
            "|{40055000, 36971009}| 15777000|0.33263162711941974| 0.2840330063400889| 6.844466402241788| 0.3326244105954414|\n",
            "|{444814009, 16286...|196416002|0.05744179941922465|0.04988347702800489| 7.599808058723829|0.07429495699201345|\n",
            "|{72892002, 44465007}|444814009| 0.6616682286785379| 0.5585304927407141|  6.41538446294208| 0.6616603418231526|\n",
            "|{713197008, 27173...| 15777000| 0.6977616685743355| 0.6491630477950047|14.357643434833763|  0.697752585908956|\n",
            "|{195662009, 72892...|198992004|0.11087883778875797|0.10776004394915077| 35.55183301334298| 0.1350155042053926|\n",
            "|{237602007, 59621...| 44054006| 0.9986836233824186|  0.988080163246757| 94.18469165773902|  0.998683609502661|\n",
            "|{444814009, 27173...|302870006|0.17276554062327157|0.16234231160315413| 16.57504985161738|0.17276389006604145|\n",
            "|{444814009, 74400...|428251008|                1.0| 0.9926024316935731| 135.1795561159216|                1.0|\n",
            "+--------------------+---------+-------------------+-------------------+------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Convert RDDs to DataFrames and join them on columns 'X' and 'Y'\n",
        "XYZ_results = (XYZ_confidence.toDF(['(X, Y)', 'Z', 'confidence'])\n",
        "            .join(XYZ_interest.toDF(['(X, Y)', 'Z', 'interest']), on = ['(X, Y)', 'Z'])\n",
        "            .join(XYZ_lift.toDF(['(X, Y)', 'Z', 'lift']), on = ['(X, Y)', 'Z'])\n",
        "            .join(XYZ_stdLift.toDF(['(X, Y)', 'Z', 'std_lift']), on = ['(X, Y)', 'Z'])\n",
        "            )\n",
        "\n",
        "XYZ_results.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---------+----------+------------------+------------------+--------+\n",
            "|              (X, Y)|        Z|confidence|          interest|              lift|std_lift|\n",
            "+--------------------+---------+----------+------------------+------------------+--------+\n",
            "|{162864005, 12861...|703151001|       1.0|0.9941441346910744|170.76895509802543|     1.0|\n",
            "|{368581000119106,...| 44054006|       1.0|0.9893965398643384| 94.30883760639537|     1.0|\n",
            "|{713197008, 44481...| 68496003|       1.0|0.9882606997822825| 85.18395317045814|     1.0|\n",
            "|{195662009, 79586...| 19169002|       1.0|0.9723077771372304|36.111221730214865|     1.0|\n",
            "|{127013003, 10509...| 44054006|       1.0|0.9893965398643384| 94.30883760639537|     1.0|\n",
            "|{444814009, 74400...|428251008|       1.0|0.9926024316935731| 135.1795561159216|     1.0|\n",
            "|{431855005, 44054...|127013003|       1.0|0.9952529538220175|210.65731457135425|     1.0|\n",
            "|{1551000119108, 2...|422034002|       1.0|0.9971942102742983| 356.4058955807587|     1.0|\n",
            "|{368581000119106,...| 44054006|       1.0|0.9893965398643384| 94.30883760639537|     1.0|\n",
            "|{237602007, 44054...|302870006|       1.0|0.9895767709798826| 95.93955942730813|     1.0|\n",
            "|{302870006, 80394...|237602007|       1.0|0.9897958189947411| 97.99904563478728|     1.0|\n",
            "|{444814009, 42203...| 44054006|       1.0|0.9893965398643384| 94.30883760639537|     1.0|\n",
            "|{127013003, 59621...| 44054006|       1.0|0.9893965398643384| 94.30883760639537|     1.0|\n",
            "|{422034002, 80394...|302870006|       1.0|0.9895767709798826| 95.93955942730813|     1.0|\n",
            "|{444814009, 79586...| 19169002|       1.0|0.9723077771372304|36.111221730214865|     1.0|\n",
            "|{80394007, 59621000}|302870006|       1.0|0.9895767709798826| 95.93955942730813|     1.0|\n",
            "|{422034002, 80394...|237602007|       1.0|0.9897958189947411| 97.99904563478728|     1.0|\n",
            "|{429007001, 16286...|410429000|       1.0|0.9940804914356616| 168.9329425122228|     1.0|\n",
            "|{80394007, 44054006}|302870006|       1.0|0.9895767709798826| 95.93955942730813|     1.0|\n",
            "|{444814009, 12701...| 44054006|       1.0|0.9893965398643384| 94.30883760639537|     1.0|\n",
            "+--------------------+---------+----------+------------------+------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Select std_lift column and filter using those values\n",
        "XYZ_results_filtered = XYZ_results.filter(XYZ_results.std_lift >= lift_threshold)\n",
        "\n",
        "# Sort in descending order\n",
        "XYZ_results_filtered = XYZ_results_filtered.sort(XYZ_results_filtered.std_lift.desc())\n",
        "\n",
        "XYZ_rules = XYZ_results_filtered.collect()\n",
        "\n",
        "XYZ_results_filtered.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And finally, just like for $\\{X\\}\\rightarrow\\{Y\\}$ we can save the rules to a text file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('XYZ_rules_with_codes.txt', 'w') as f:\n",
        "  f.write(f\"{'X, Y':<40} {'->':<5} {'Z':<15}\\t{'Std Lift':<20}\\t{'Lift':<20}\\t{'Confidence':<20}\\t{'Interest':<20}\\n\")\n",
        "  for rule in XYZ_rules:\n",
        "    X, Y = rule['(X, Y)']\n",
        "    f.write(f\"{X:<20}{Y:<20} {'->':<5} {rule['Z']:<15}\\t{rule['std_lift']:<20}\\t{rule['lift']:<20}\\t{rule['confidence']:<20}\\t{rule['interest']:<20}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('XYZ_rules_with_description.txt', 'w') as f:\n",
        "  f.write(f\"{'X, Y':<180} {'->':<5} {'Z':<80}\\t{'Std Lift':<20}\\t{'Lift':<20}\\t{'Confidence':<20}\\t{'Interest':<20}\\n\")\n",
        "  for rule in XYZ_rules:\n",
        "    X, Y = rule['(X, Y)']\n",
        "    f.write(f\"{code_to_desc[str(X)]:<90}{code_to_desc[str(Y)]:<90} {'->':<5} {(code_to_desc[str(rule['Z'])]):<80}\\t{rule['std_lift']:<20}\\t{rule['lift']:<20}\\t{rule['confidence']:<20}\\t{rule['interest']:<20}\\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also save the 10 most frequent itemsets for $k=1,2,3$ to a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('most_frequent_itemsets.txt', 'w') as f:\n",
        "    # k = 1\n",
        "    f.write(f\"10 most frequent items for k = 1:\\n{'Code':<42} {'Conditions':<120} {'Count':<15}\\n\")\n",
        "    for item in frequent_items.take(10):\n",
        "        f.write(f'{item[0]:<42} {code_to_desc[str(item[0])]:<120} {item[1]:<15}\\n')\n",
        "    # k = 2\n",
        "    f.write(f\"\\n10 most frequent items for k = 2:\\n{'Code':<42} {'Conditions':<120} {'Count':<15}\\n\")\n",
        "    for pair in most_frequent_pairs:\n",
        "        f.write(f'{pair[0][0]:<14}{pair[0][1]:<28} {code_to_desc[str(pair[0][0])]:<60}{code_to_desc[str(pair[0][1])]:<60} {pair[1]:<15}\\n')\n",
        "    # k = 3\n",
        "    f.write(f\"\\n10 most frequent items for k = 3:\\n{'Code':<42} {'Conditions':<120} {'Count':<15}\\n\")\n",
        "    for triplet in most_frequent_triplets:\n",
        "        f.write(f\"\"\"{triplet[0][0]:<14}{triplet[0][1]:<14}{triplet[0][1]:<14} {code_to_desc[str(triplet[0][0])]:<40}{code_to_desc[str(triplet[0][1])]:<40}{code_to_desc[str(triplet[0][2])]:<40} {triplet[1]:<15}\\n\"\"\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
