{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 - Exponentially Decaying Window\n",
    "\n",
    "Francisco Marques 97639 Data Science\n",
    "\n",
    "Here we apply the Exponentially Decaying Window method to a stream of events (letters) generated by ''simple_socket_server_csv.py' using Spark's Structured Streaming. I was not able to use the console sink format in Jupyter Notebook so all of the code was tested in a regular Python script.\n",
    "\n",
    "### Install and load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col, lit, window\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "from pyspark.sql.functions import round as spark_round\n",
    "from pyspark.sql.types import TimestampType\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10 # window size\n",
    "slide = 1 # slide duration (e.g. with window_size = 10 â†’ 0-10, 1-11, 2-12, etc...)\n",
    "decay = 0.1 # decay constant"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark session\n",
    "spark = SparkSession.builder.appName(\"Assignment 3 - Exponentially Decaying Window\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to server via socket\n",
    "\n",
    "Default values: host = 'localhost'; port = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socket_df = (spark.readStream.format(\"socket\") \n",
    "        .option(\"host\", 'localhost') \n",
    "        .option(\"port\", 9999)\n",
    "        .load())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Dataframe with a timestamp (TimestampType) column and value (str) column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df = (socket_df.withColumn('tmp', split(socket_df.value, ',')) # split by comma (csv)\n",
    "            .withColumn('event_time', col('tmp').getItem(0).cast(TimestampType())) # create column with timestamps\n",
    "            .withColumn('event', col('tmp').getItem(1)) # column with events\n",
    "            .drop(col(\"tmp\")).drop(col(\"value\"))\n",
    "            .withColumn(\"count\", lit(1))) # column for initial weighted sum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the schema of latest Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print schema to dataframe\n",
    "split_df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new Dataframe with sliding window that also computes the weighted sum, making up the Exponentially Decaying Window method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_df = (split_df.withColumn(\"decay_factor\", lit(1 - decay)) # create decay factor column\n",
    "    .withColumn(\"weighted_sum\", (col(\"count\") * col(\"decay_factor\")) + 1)  # compute decay_factor for each row\n",
    "    .groupBy(window(col(\"event_time\"), f\"{window_size} seconds\", slideDuration = f\"{slide} seconds\") # group by sliding time window\n",
    "            .alias(\"window\"), col(\"event\")) \n",
    "    .agg(spark_sum(col(\"weighted_sum\")).alias(\"total_weighted_sum\"), col(\"event\")) # aggregate by summing up every weighted_sum and by event\n",
    "    .withColumn(\"start\", col(\"window\").getField(\"start\"))\n",
    "    .withColumn(\"end\", col(\"window\").getField(\"end\"))\n",
    "    .withColumn(\"total_weighted_sum\", spark_round(col(\"total_weighted_sum\"), 4))\n",
    "    .select(\"event\", \"start\", \"end\", \"total_weighted_sum\")\\\n",
    "    .orderBy(col(\"total_weighted_sum\").desc(), col(\"window.start\"))) # order by descending total_weighted_sum "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once again print the schema of this DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create query to output results to console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output to console\n",
    "window_query = (windowed_df.writeStream\n",
    "                .outputMode(\"complete\") \n",
    "                .format(\"console\") \n",
    "                .start())\n",
    "\n",
    "# Wait for the queries to terminate\n",
    "window_query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
