{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 - DGIM on stream of bits\n",
    "\n",
    "Francisco Marques 97639 Data Science\n",
    "\n",
    "Here we apply the DGIM method to a synthethic stream of bits generated by ''simple_socket_server_bits.py' using Spark's Structured Streaming. I was not able to use the console sink format in Jupyter Notebook so all of the code was tested in a regular Python script.\n",
    "\n",
    "### Install and load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col\n",
    "from dgim import * # custom class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # window size\n",
    "k = 50 # last k entries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark session\n",
    "spark = SparkSession.builder.appName(\"Assignment 3 - DGIM\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to server via socket\n",
    "\n",
    "Default values: host = 'localhost'; port = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socket_df = (spark.readStream.format(\"socket\") \n",
    "        .option(\"host\", 'localhost') \n",
    "        .option(\"port\", 9999)\n",
    "        .load())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Dataframe with a timestamp (int) column and value (bit) column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df = (socket_df.withColumn(\"tmp\", split(col(\"value\"), \",\")) # split values in column\n",
    "            .withColumn(\"timestamp\", col(\"tmp\").getItem(0)) # time column \n",
    "            .withColumn(\"value\", col(\"tmp\").getItem(1)) # bit column\n",
    "            .drop(col(\"tmp\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the schema of latest Dataframe and initialize DGIM object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print schema to dataframe\n",
    "split_df.printSchema()\n",
    "\n",
    "# Initialize DGIM object\n",
    "dgim = DGIM(N, k)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to update the DGIM object every batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_cumulative_sum(batch_df, batch_id):\n",
    "        \"Update DGIM object cumulative and real sum with each batch\"\n",
    "        \n",
    "        print(f\"Batch: {batch_id}\\n\")\n",
    "        t_start = dgim.stream_timestamp # starting time of the batch\n",
    "        rows = batch_df.toLocalIterator() # convert batch rows to generator\n",
    "        \n",
    "        for row in rows:\n",
    "            bit = row[\"value\"]\n",
    "            dgim.update(bit)\n",
    "        dgim.estimated_count += dgim.count()\n",
    "        t_end = dgim.stream_timestamp # ending time of the batch\n",
    "        \n",
    "        new_df = spark.createDataFrame([(t_start, t_end, dgim.estimated_count, dgim.real_count)], \n",
    "                                    ['t_start', 't_end', 'estimated_sum', 'real_sum'])\n",
    "        new_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create query to apply the previous function to each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply update_cumulative_sum to each batch\n",
    "df_dgim = (split_df.writeStream\n",
    "        .foreachBatch(update_cumulative_sum)\n",
    "        .start())\n",
    "\n",
    "df_dgim.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
