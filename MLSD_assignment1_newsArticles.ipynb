{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Large Scale Data Mining assignment 1\n",
        "### Similar items\n",
        "\n",
        "Francisco Marques 97639 - Mestrado em CiÃªncia de Dados\n",
        "franciscocmarques@ua.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3RPBVDRWmXz",
        "outputId": "77576b44-d3f4-457a-b5e0-95d6f6ee4f0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.3.2)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyspark) (0.10.9.5)\n",
            "Requirement already satisfied: sympy in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.11.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy) (1.3.0)\n",
            "Requirement already satisfied: xxhash in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.2.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.65.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\franc\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n",
            "Requirement already satisfied: numpy in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.24.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scipy) (1.24.2)\n",
            "Requirement already satisfied: nltk in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: joblib in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2023.3.23)\n",
            "Requirement already satisfied: colorama in c:\\users\\franc\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark\n",
        "%pip install sympy\n",
        "%pip install xxhash \n",
        "%pip install tqdm \n",
        "%pip install numpy\n",
        "%pip install scipy\n",
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCZJv8krMdR9",
        "outputId": "2f4a0d7c-cfaa-482d-d9e1-d0901d153732"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.3.2\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "sc = SparkContext(appName=\"Assignment1 - News Articles\")\n",
        "print(sc.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "irX1vewRO2TY",
        "outputId": "d40ecd1f-a995-4259-da4b-f570adc7b183"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://DESKTOP-PI3ARK7.Home:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Test</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x1c3f0f67c70>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark = SparkSession.builder.appName(\"Assignment1 - News Articles\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "P-osjw1_sVWZ"
      },
      "source": [
        "### 2.1 Tuning $b$ bands and $r$ rows \n",
        "\n",
        "Computed the number of bands and rows in order to obtain at least $90\\%$ of pairs with $85\\%$ similarity and less than $5\\%$ of pairs with $60\\%$ similarity. Although we obtained around $79\\%$ similarity, we can consider this a good value thanks to the accuracy vs complexity trade off. Reaching a similarity threshold closer to $85\\%$ would increase the computational load exponentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "05srn5UKsVc3"
      },
      "outputs": [],
      "source": [
        "def tune_params(bands, rows, sim_threshold):\n",
        "\n",
        "  sim = (1/bands)**(1/rows)\n",
        "  print(f\"We obtained a s_threshold of {sim}, should be close to {sim_threshold}, with {rows} rows and {bands} bands.\")\n",
        "\n",
        "  sim_probability = 0.85\n",
        "  p1 = sim_probability ** rows\n",
        "  print(f\"Probability of C1 and C2 identical in a given band = s^r = {p1}\")\n",
        "\n",
        "  p2 = (1 - p1) ** bands\n",
        "  print(f\"Probability C1, C2 are not similar in all of the {bands} bands = (1-s^r)^b = {p2}\")\n",
        "  if p2 < .1:\n",
        "      print(\"Condition 1 obtained\\n\")\n",
        "\n",
        "  not_sim_probability = 0.6\n",
        "  p3 = not_sim_probability ** rows\n",
        "  print(f\"Probability C1, C2 are identical in a given band = s^r = {p3}\")\n",
        "\n",
        "  p4 = 1-(1-p3) ** bands\n",
        "  print(f\"Probability C1, C2 identical in at least 1 of 20 bands: 1-(1-s^r)^b = {p4}\")\n",
        "  if p4 < 0.05:\n",
        "      print(\"Condition 2 obtained\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6E1-yQMUJTS",
        "outputId": "e2d7c548-1d7b-4d4d-d664-f11b39172d0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We obtained a s_threshold of 0.7920132050096789, should be close to 0.85, with 11 rows and 13 bands.\n",
            "Probability of C1 and C2 identical in a given band = s^r = 0.1673432436896142\n",
            "Probability C1, C2 are not similar in all of the 13 bands = (1-s^r)^b = 0.09248219617246255\n",
            "Condition 1 obtained\n",
            "\n",
            "Probability C1, C2 are identical in a given band = s^r = 0.0036279705599999985\n",
            "Probability C1, C2 identical in at least 1 of 20 bands: 1-(1-s^r)^b = 0.0461505019889491\n",
            "Condition 2 obtained\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(True, True, 0.7920132050096789)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tune_params(13, 11, 0.85)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N_rMEcLgsV29"
      },
      "source": [
        "### 2.2 Function that, given an article, finds every article at least $85\\%$ similar.\n",
        "\n",
        "For this, we need to implement the full Locally-Sensitive Hashing (LSH) algorithm, starting with the Shingling and hashing the resulting shingles.\n",
        "\n",
        "Here we can see the structure of each JSON entry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXS8pGMFWy54",
        "outputId": "f1462522-0f7d-4fd9-f038-cb0eb3a436d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- tweet_id: string (nullable = true)\n",
            " |-- url: string (nullable = true)\n",
            "\n",
            "+--------------------+-------------------+--------------------+\n",
            "|                text|           tweet_id|                 url|\n",
            "+--------------------+-------------------+--------------------+\n",
            "|Os especialistas ...|1215246971079929857|https://www.dn.pt...|\n",
            "|Os leitores sÃ£o a...|1215233921425854465|https://publico.p...|\n",
            "|Os leitores sÃ£o a...|1223386542145724416|https://www.publi...|\n",
            "|Segundo a Associa...|1223380605897003008|https://www.cmjor...|\n",
            "|Os leitores sÃ£o a...|1223378524830097408|https://www.publi...|\n",
            "|Os leitores sÃ£o a...|1223375795999191040|https://publico.p...|\n",
            "|(Enviada diariame...|1223373056229494784|https://www.cmjor...|\n",
            "|De acordo com dad...|1223371884273766402|https://www.cmjor...|\n",
            "|                    |1223368728940818440|https://www.publi...|\n",
            "|Os leitores sÃ£o a...|1223368343710838784|https://publico.p...|\n",
            "|(Enviada diariame...|1223355638501257218|https://www.cmjor...|\n",
            "|Um cidadÃ£o italia...|1223354127184515073|https://www.jn.pt...|\n",
            "|Dono de empresa d...|1223347891051298822|https://www.cmjor...|\n",
            "|(Enviada diariame...|1223339972457127937|https://www.cmjor...|\n",
            "|Um cidadÃ£o italia...|1223335102681436164|https://www.jn.pt...|\n",
            "|Os leitores sÃ£o a...|1223332850948288514|https://publico.p...|\n",
            "|Siga-nos Jornalis...|1223326482279342086|http://expresso.p...|\n",
            "|A OrganizaÃ§Ã£o Mun...|1223325243210633218|https://www.cmjor...|\n",
            "|Os leitores sÃ£o a...|1223320219361456130|https://publico.p...|\n",
            "|Os leitores sÃ£o a...|1223319115034701825|https://www.publi...|\n",
            "+--------------------+-------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.json('data/covid_news_full.json.bz2')\n",
        "df.printSchema()\n",
        "df.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X615NyrYIhbM"
      },
      "source": [
        "#### Shingling\n",
        "\n",
        "Before we can shingle the documents, we need to clean the entries. For this, every punctuation and stopword was removed. As this algorithm can take a long time, we make use of *tqdm*'s progress bar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SYYoGWMPGNKt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm # progress bar\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from sympy import nextprime\n",
        "\n",
        "np.set_printoptions(threshold=np.inf, precision=0, suppress=True) # prevents scientific notation when printing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After cleaning the entries, we remove any entry smaller than the shingle size. Since we considered a somewhat large shingle size ($k=10$) in this notebook, entries smaller than this, were in the few cases that I looked into, always the same (\"(Enviada diariamente)\" appeared many times).\n",
        "\n",
        "Then the shingling itself, is quite simple. We made each document's shingles into a set to remove any duplicate as duplicates wont aid in finding similar items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ffnyar3sXS-4"
      },
      "outputs": [],
      "source": [
        "shingle_size = 10\n",
        "\n",
        "punctuation_table = str.maketrans(dict.fromkeys(punctuation, '')) # Table to remove punctuation from text\n",
        "stop_words = set(stopwords.words('portuguese')) # Set with stop words\n",
        "\n",
        "news_rdd = df.rdd.map(lambda item: (item['tweet_id'], item['text'])) # Removes url\n",
        "\n",
        "shingle_rdd = (news_rdd.map(lambda item: (item[0], item[1].translate(punctuation_table).split()))\n",
        "                   # Remove punctuation and stop words\n",
        "                .map(lambda item: (item[0], [w for w in item[1] if w.lower() not in stop_words]))\n",
        "                # Filter out articles with less than 'shingle_size' words\n",
        "                .filter(lambda item: len(item[1]) >= shingle_size) \n",
        "                # Shingle the documents\n",
        "                .map(lambda item: (item[0], set([tuple(item[1][i:i + shingle_size]) for i in range(len(item[1]) - shingle_size+1)])))\n",
        "                ) \n",
        "\n",
        "# number of total distinct shingles to use in the hash functions\n",
        "num_shingles = shingle_rdd.flatMap(lambda item: item[1]).distinct().count()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we define the hash function and the permutations, which are constant throughout the algorithm. We use *sympy*.nextprime(x) to find the smallest prime greater than x. The hash function is the same as in the class slides: $((a*h(x)+b)\\mod{p})\\mod{N}$ where $N$ is the number of shingles, and $p$ the next smallest prime number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tOMXbOI2pqN4"
      },
      "outputs": [],
      "source": [
        "p = nextprime(num_shingles)\n",
        "\n",
        "num_permutations = 13 * 11 # bands x rows obtained from 2.1. b*r=n; n is the number of permutations\n",
        "a = np.random.randint(1, p, size = (num_permutations))\n",
        "b = np.random.randint(0, p, size = (num_permutations))\n",
        "\n",
        "a_bc = sc.broadcast(a)\n",
        "b_bc = sc.broadcast(b)\n",
        "\n",
        "def hash_function(x, i):\n",
        "  return ((a_bc.value[i] * hash(x) + b_bc.value[i]) % p) % num_shingles"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With this we hash every shingle in each document and turn that list to a set, in order to remove duplicates. \n",
        "\n",
        "We also print some variables used throughout the algorithm such as the total number of unique shingles and number of articles. The original size is around 59 thousand, so with the previous filter we removed alot of small entries which can help speed up the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "H14KXriepxea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique shingles: 7254120\n",
            "Number of documents: 54446\n"
          ]
        }
      ],
      "source": [
        "# Hash shingles and convert list to set to remove duplicates\n",
        "hashed_shingles = shingle_rdd.map(lambda item: (item[0], set([hash(shingle) for shingle in item[1]])))\n",
        "num_docs = hashed_shingles.count()\n",
        "\n",
        "print(f'Number of unique shingles: {num_shingles}')\n",
        "print(f'Number of documents: {num_docs}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w_UNonVy0tgo"
      },
      "source": [
        "#### MinHashing\n",
        "\n",
        "Now that we have our shingles, we can perform the minhashing and create our signature matrix. This matrix will have the shape ('num_permutations', 'num_articles'), i.e., (143, 54446).\n",
        "\n",
        "For this, we use the permutation values and hash every shingle in each document, then we append the minimum value to the 'minhash_values' list, and repeat until it is done 'num_permutations' times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kaHYHIGKIoMK"
      },
      "outputs": [],
      "source": [
        "def minhash(document):\n",
        "    \"\"\"Applies minhashing to document.\"\"\"\n",
        "    minhash_values = []\n",
        "    \n",
        "    # For each permutation, get hash for every shingle and keep lowest\n",
        "    for i in range(num_permutations):\n",
        "        permuted_hash = [hash_function(shingle, i) for shingle in document]\n",
        "        minhash_values.append(min(permuted_hash))\n",
        "    \n",
        "    return minhash_values"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the speed up method of splitting the hashing into blocks, we split it into blocks of 500 entries, or 109 blocks.\n",
        "\n",
        "We also repartition the shingles RDD (hashed shingles), and cache it since it will be accesses 109 times.\n",
        "\n",
        "Then, every time a block is completed, we append its transpose (a column), to 'signature_matrix_blocks', which after every block is completed, concatenates them, creating our signature matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "K-kPi1xb0xmF",
        "outputId": "acecaa74-e635-46f4-aab8-e9380b32405b"
      },
      "outputs": [],
      "source": [
        "def signature_matrix(shingles, num_docs, block_size):\n",
        "    \"\"\"Create signature matrix using minhash.\"\"\"\n",
        "    signature_matrix = []\n",
        "    signature_matrix_blocks = []\n",
        "    \n",
        "    # Repartition RDD to lower bound (2 * cores) to reduce overhead and speed up process\n",
        "    shingles_with_ids = shingles.zipWithIndex().map(lambda x: (x[1], x[0])).repartition(32).cache()\n",
        "\n",
        "    # Process the data in blocks\n",
        "    for i in tqdm(range(0, num_docs, block_size), desc = 'Hashing blocks'):\n",
        "        # Get the next block of data\n",
        "        block = shingles_with_ids.filter(lambda x: x[0] >= i and x[0] < i + block_size).map(lambda doc: doc[1][1])\n",
        "        \n",
        "        # Compute the MinHash values for each document in the block\n",
        "        minhash_block = block.map(lambda doc: minhash(doc))\n",
        "        signature_matrix_blocks.append(np.array(minhash_block.collect()).T)\n",
        "\n",
        "    signature_matrix = np.concatenate(signature_matrix_blocks, axis = 1)\n",
        "\n",
        "    return signature_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Hashing blocks: 100%|ââââââââââ| 109/109 [31:00<00:00, 17.07s/it]\n"
          ]
        }
      ],
      "source": [
        "sig_mat = signature_matrix(hashed_shingles, num_docs, 500)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LSH\n",
        "\n",
        "Finally, we can generate our candidate pairs with LSH, but first we still need to split our entries into buckets.\n",
        "\n",
        "For this we split our signature matrix into $b$ (13) bands, and in each band hash each column to one hash value, this is, we group the rows of a column in a band into a tuple, and hash that tuple. Then iterating over the hashes that were created, we split them into buckets, where the key is the hash value, this way if they have the same hash value, it means that the documents in that bucket are potential similar. Lastly, we add these buckets to a list, and repeat for the remaining bands, keeping each band's buckets *away* from the remaining. \n",
        "\n",
        "After every entry is distributed in the buckets, we go over each band's buckets and create every possible pair for buckets with at least 2 elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "IV20WfiVqp2w"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def lsh(signature_matrix, num_docs, num_bands):\n",
        "    # Split signature matrix into bands\n",
        "    bands = np.split(signature_matrix, num_bands)\n",
        "    buckets = []\n",
        "    \n",
        "    # Hash buckets by bands\n",
        "    for band in tqdm(bands, desc = 'Bucket hashing'):\n",
        "        # Hash each column r rows at a time\n",
        "        band_hashes = [hash_function(tuple(band[:, j]), 0) for j in range(num_docs)]\n",
        "        bucket_dict = {}\n",
        "        \n",
        "        # Distribute hashes over buckets\n",
        "        for j, b_hash in enumerate(band_hashes):\n",
        "            if b_hash not in bucket_dict:\n",
        "                bucket_dict[b_hash] = []\n",
        "            bucket_dict[b_hash].append(j)\n",
        "        buckets.append(bucket_dict)\n",
        "    \n",
        "    # Generate candidate pairs if they are in the same bucket\n",
        "    candidate_pairs = set()\n",
        "    for bucket in tqdm(buckets, desc = 'Candidate pairs generation'):\n",
        "        for b_hash, docs in list(bucket.items()):\n",
        "            if len(docs) < 2:\n",
        "                continue\n",
        "            for pair in combinations(docs, 2):\n",
        "                candidate_pairs.add(pair)\n",
        "\n",
        "    return list(candidate_pairs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we call the function and print the total number of candidates, and we can see that we have many candidates, which we will confirm if they are false positives or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iquAgD9UeV2M",
        "outputId": "dc970520-b943-4d64-e705-a1c0a3e6500c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Bucket hashing: 100%|ââââââââââ| 13/13 [00:03<00:00,  4.21it/s]\n",
            "Candidate pairs generation: 100%|ââââââââââ| 13/13 [00:00<00:00, 15.43it/s]\n"
          ]
        }
      ],
      "source": [
        "candidate_pairs = lsh(sig_mat, num_docs, 13)\n",
        "\n",
        "print(f'Total candidate pairs found: {len(candidate_pairs)}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we create out *jaccard* similarity function, and the function to find similar items in the candidate pairs. Since this function can be considered brute force since we're making sure if they are similar or not, it will take much longer than the previous algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def jaccard(set1: set[int], set2: set[int]):\n",
        "    \"\"\"Jaccard similarity between set1 and set2.\"\"\"\n",
        "    return len(set1.intersection(set2)) / len(set1.union(set2))\n",
        "\n",
        "def get_similar(id, candidate_pairs, threshold):\n",
        "    \"\"\"Find similar articles to the input based on the Jaccard similarity.\"\"\"\n",
        "    candidate_documents = set([x for pair in candidate_pairs for x in pair]) # unpack pairs into a set\n",
        "    \n",
        "    # Map each document to an id, then filters non candidate documents\n",
        "    candidates_rdd = hashed_shingles.zipWithIndex().filter(lambda item: item[1] in candidate_documents).collect()\n",
        "    \n",
        "    # Dictionary with index: doc_id\n",
        "    candidates = {i: doc_id for doc_id, i in candidates_rdd}\n",
        "    similar_articles = []\n",
        "    \n",
        "    # Compute similarity of every pair\n",
        "    for pair in candidate_pairs:\n",
        "        if id in pair:\n",
        "            sim = jaccard(candidates[pair[0]][1], candidates[pair[1]][1])\n",
        "            if sim >= threshold: # keep pairs that verify the similarity threshold\n",
        "                suggestion_id = set(pair) - set((id,)) # Add suggested article id to list\n",
        "                similar_articles.append(candidates[suggestion_id.pop()][0]) \n",
        "                \n",
        "    return similar_articles"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Performance Evaluation\n",
        "Now we must test the performance of our implementation, by running for several random samples and obtain false positives and false negatives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "Similar articles found: 6\n",
            "['1362093434945888256', '1465444548742725636', '1407372196125843461', '1398152355825209345', '1240979985709621250', '1398873353738412032']\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n",
            "No similar news articles were found!\n"
          ]
        }
      ],
      "source": [
        "# Dictionary with articles index and id\n",
        "article_id = {key: id[0] for id, key in hashed_shingles.zipWithIndex().collect()}\n",
        "\n",
        "# Unpack random 100 pairs\n",
        "random_candidates = set([candidate_pairs[idx][0] for idx in np.random.choice(len(candidate_pairs), 100)])\n",
        "\n",
        "# Make sure there are 100 candidates\n",
        "while len(random_candidates) < 100:\n",
        "    random_candidates.add(candidate_pairs[np.random.randint(0, len(candidate_pairs), 1)[0]][0])\n",
        "\n",
        "suggestions = {id: [] for id in random_candidates}\n",
        "\n",
        "# Verify how many candidate pairs are similar, save to dictionary regardless of being similar or not\n",
        "for candidate in random_candidates:\n",
        "    similar_articles = get_similar(candidate, candidate_pairs, 0.85)\n",
        "    candidate_id = article_id[candidate] # convert the index to the article id\n",
        "    suggestions[candidate_id].extend(similar_articles)\n",
        "    \n",
        "    if len(suggestions[candidate]) == 0:\n",
        "        print('No similar news articles were found!')\n",
        "\n",
        "    else:\n",
        "        print(f'Similar articles found: {len(suggestions[candidate])}')\n",
        "        print(suggestions[candidate])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
