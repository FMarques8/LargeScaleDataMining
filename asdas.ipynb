{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_HOME']=r\"C:\\PySpark\\spark-3.3.2-bin-hadoop3\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"abc\").set(\"spark.driver.memory\", \"6g\").set(\"spark.executor.memory\", \"6g\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('90f0b8d0-3888-415f-8234-d68f7beab894', '195662009'),\n",
    " ('b039fdd7-d57e-4b1d-be07-41b2f8bd38cc', '195662009'),\n",
    " ('d8df898b-6d23-4b14-9acc-732bcd9b966e', '59621000'),\n",
    " ('429f9d06-936e-4cac-8b35-bd0beaf9fd30', '10509002'),\n",
    " ('a95aa3c0-a7d9-4b55-a7b7-3b88e74ecdec', '44465007'),\n",
    " ('b5573ec3-a770-4397-9ea6-c5b328ce1690', '72892002'),\n",
    " ('3c0e4922-1d5a-4af8-8fbf-363889c44af8', '431855005'),\n",
    " ('4b016f9a-96c0-4305-a93e-7930703fc885', '444814009'),\n",
    " ('81887248-804a-4d2c-9305-e64291dcc1e6', '59621000'),\n",
    " ('50d43617-d19d-401f-8686-8b62fedeaf47', '196416002'),\n",
    " ('b026baab-096a-4d94-b784-9941ef67ad2f', '87433001'),\n",
    " ('b026baab-096a-4d94-b784-9941ef67ad2f', '428251008')], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b039fdd7-d57e-4b1d-be07-41b2f8bd38cc', ('195662009',)),\n",
       " ('4b016f9a-96c0-4305-a93e-7930703fc885', ('444814009',)),\n",
       " ('a95aa3c0-a7d9-4b55-a7b7-3b88e74ecdec', ('44465007',)),\n",
       " ('b5573ec3-a770-4397-9ea6-c5b328ce1690', ('72892002',)),\n",
       " ('429f9d06-936e-4cac-8b35-bd0beaf9fd30', ('10509002',)),\n",
       " ('81887248-804a-4d2c-9305-e64291dcc1e6', ('59621000',)),\n",
       " ('b026baab-096a-4d94-b784-9941ef67ad2f', ('87433001', '428251008')),\n",
       " ('3c0e4922-1d5a-4af8-8fbf-363889c44af8', ('431855005',)),\n",
       " ('50d43617-d19d-401f-8686-8b62fedeaf47', ('196416002',)),\n",
       " ('90f0b8d0-3888-415f-8234-d68f7beab894', ('195662009',)),\n",
       " ('d8df898b-6d23-4b14-9acc-732bcd9b966e', ('59621000',))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupedRdd = rdd.groupByKey().mapValues(tuple)\n",
    "groupedRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['195662009',\n",
       " 'b039fdd7-d57e-4b1d-be07-41b2f8bd38cc',\n",
       " '4b016f9a-96c0-4305-a93e-7930703fc885',\n",
       " '87433001',\n",
       " '431855005',\n",
       " '196416002',\n",
       " 'a95aa3c0-a7d9-4b55-a7b7-3b88e74ecdec',\n",
       " 'b5573ec3-a770-4397-9ea6-c5b328ce1690',\n",
       " '59621000',\n",
       " '429f9d06-936e-4cac-8b35-bd0beaf9fd30',\n",
       " '10509002',\n",
       " '81887248-804a-4d2c-9305-e64291dcc1e6',\n",
       " 'b026baab-096a-4d94-b784-9941ef67ad2f',\n",
       " '3c0e4922-1d5a-4af8-8fbf-363889c44af8',\n",
       " '50d43617-d19d-401f-8686-8b62fedeaf47',\n",
       " '428251008',\n",
       " '72892002',\n",
       " '444814009',\n",
       " '90f0b8d0-3888-415f-8234-d68f7beab894',\n",
       " 'd8df898b-6d23-4b14-9acc-732bcd9b966e',\n",
       " '44465007']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueRdd = rdd.flatMap(lambda item: item).distinct()\n",
    "uniqueRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinedRdd.To"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedRdd = groupedRdd.cartesian(uniqueRdd).filter(lambda item: item[0] != item[1])\n",
    "\n",
    "\n",
    "\n",
    "# combinedRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_duplicate_pairs(record):\n",
    "#     \"\"\"Remove one of two duplicated pairs from using cartesian on the RDD.\n",
    "    \n",
    "#     Ex.: [a,b,c].cartesian(...) → [(a,b), (b,a), (c,a),...].map(function) → [(a,b), (b,c), (a,c)]\n",
    "#     \"\"\"\n",
    "#     if(isinstance(record[0], tuple)):\n",
    "#         x1 = record[0]\n",
    "#         x2 = record[1]\n",
    "#     else:\n",
    "#         x1 = [record[0]]\n",
    "#         x2 = record[1]\n",
    "\n",
    "#     if(any(x == x2 for x in x1) == False):\n",
    "#         a = list(x1)\n",
    "#         a.append(x2)\n",
    "#         a.sort()\n",
    "#         result = tuple(a)\n",
    "#         return result \n",
    "#     else:\n",
    "#         return x1\n",
    "\n",
    "def sort_values(item):\n",
    "    \n",
    "    if isinstance(item[1], tuple):\n",
    "        return (item[1], item[0])\n",
    "    \n",
    "    else: \n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('d8df898b-6d23-4b14-9acc-732bcd9b966e', ('59621000',)),\n",
       "  'd8df898b-6d23-4b14-9acc-732bcd9b966e'),\n",
       " (('d8df898b-6d23-4b14-9acc-732bcd9b966e', ('59621000',)),\n",
       "  '429f9d06-936e-4cac-8b35-bd0beaf9fd30'),\n",
       " (('d8df898b-6d23-4b14-9acc-732bcd9b966e', ('59621000',)),\n",
       "  '81887248-804a-4d2c-9305-e64291dcc1e6'),\n",
       " (('d8df898b-6d23-4b14-9acc-732bcd9b966e', ('59621000',)),\n",
       "  'b026baab-096a-4d94-b784-9941ef67ad2f'),\n",
       " (('429f9d06-936e-4cac-8b35-bd0beaf9fd30', ('10509002',)),\n",
       "  'd8df898b-6d23-4b14-9acc-732bcd9b966e'),\n",
       " (('81887248-804a-4d2c-9305-e64291dcc1e6', ('59621000',)),\n",
       "  'd8df898b-6d23-4b14-9acc-732bcd9b966e'),\n",
       " (('429f9d06-936e-4cac-8b35-bd0beaf9fd30', ('10509002',)),\n",
       "  '429f9d06-936e-4cac-8b35-bd0beaf9fd30'),\n",
       " (('429f9d06-936e-4cac-8b35-bd0beaf9fd30', ('10509002',)),\n",
       "  '81887248-804a-4d2c-9305-e64291dcc1e6'),\n",
       " (('81887248-804a-4d2c-9305-e64291dcc1e6', ('59621000',)),\n",
       "  '429f9d06-936e-4cac-8b35-bd0beaf9fd30'),\n",
       " (('81887248-804a-4d2c-9305-e64291dcc1e6', ('59621000',)),\n",
       "  '81887248-804a-4d2c-9305-e64291dcc1e6')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinedRdd = groupedRdd.cartesian(uniqueRdd).filter(lambda item: item[0] != item[1])\n",
    "combinedRdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('d8df898b-6d23-4b14-9acc-732bcd9b966e', ('59621000',)),\n",
       "  'd8df898b-6d23-4b14-9acc-732bcd9b966e'),\n",
       " (('d8df898b-6d23-4b14-9acc-732bcd9b966e', ('59621000',)),\n",
       "  '429f9d06-936e-4cac-8b35-bd0beaf9fd30'),\n",
       " (('d8df898b-6d23-4b14-9acc-732bcd9b966e', ('59621000',)),\n",
       "  '81887248-804a-4d2c-9305-e64291dcc1e6'),\n",
       " (('d8df898b-6d23-4b14-9acc-732bcd9b966e', ('59621000',)),\n",
       "  'b026baab-096a-4d94-b784-9941ef67ad2f'),\n",
       " (('429f9d06-936e-4cac-8b35-bd0beaf9fd30', ('10509002',)),\n",
       "  'd8df898b-6d23-4b14-9acc-732bcd9b966e'),\n",
       " (('81887248-804a-4d2c-9305-e64291dcc1e6', ('59621000',)),\n",
       "  'd8df898b-6d23-4b14-9acc-732bcd9b966e'),\n",
       " (('429f9d06-936e-4cac-8b35-bd0beaf9fd30', ('10509002',)),\n",
       "  '429f9d06-936e-4cac-8b35-bd0beaf9fd30'),\n",
       " (('429f9d06-936e-4cac-8b35-bd0beaf9fd30', ('10509002',)),\n",
       "  '81887248-804a-4d2c-9305-e64291dcc1e6'),\n",
       " (('81887248-804a-4d2c-9305-e64291dcc1e6', ('59621000',)),\n",
       "  '429f9d06-936e-4cac-8b35-bd0beaf9fd30'),\n",
       " (('81887248-804a-4d2c-9305-e64291dcc1e6', ('59621000',)),\n",
       "  '81887248-804a-4d2c-9305-e64291dcc1e6'),\n",
       " (('429f9d06-936e-4cac-8b35-bd0beaf9fd30', ('10509002',)),\n",
       "  'b026baab-096a-4d94-b784-9941ef67ad2f'),\n",
       " (('81887248-804a-4d2c-9305-e64291dcc1e6', ('59621000',)),\n",
       "  'b026baab-096a-4d94-b784-9941ef67ad2f'),\n",
       " (('b026baab-096a-4d94-b784-9941ef67ad2f', ('87433001', '428251008')),\n",
       "  'd8df898b-6d23-4b14-9acc-732bcd9b966e'),\n",
       " (('b026baab-096a-4d94-b784-9941ef67ad2f', ('87433001', '428251008')),\n",
       "  '429f9d06-936e-4cac-8b35-bd0beaf9fd30'),\n",
       " (('b026baab-096a-4d94-b784-9941ef67ad2f', ('87433001', '428251008')),\n",
       "  '81887248-804a-4d2c-9305-e64291dcc1e6'),\n",
       " (('b026baab-096a-4d94-b784-9941ef67ad2f', ('87433001', '428251008')),\n",
       "  'b026baab-096a-4d94-b784-9941ef67ad2f'),\n",
       " (('d8df898b-6d23-4b14-9acc-732bcd9b966e', ('59621000',)), '44465007'),\n",
       " (('d8df898b-6d23-4b14-9acc-732bcd9b966e', ('59621000',)), '72892002'),\n",
       " (('d8df898b-6d23-4b14-9acc-732bcd9b966e', ('59621000',)), '444814009'),\n",
       " (('429f9d06-936e-4cac-8b35-bd0beaf9fd30', ('10509002',)), '44465007')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = combinedRdd.map(lambda item: sort_values(item))\n",
    "rdd.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 178.0 failed 1 times, most recent failure: Lost task 1.0 in stage 178.0 (TID 351) (192.168.62.78 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\PySpark\\spark-3.3.2-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 666, in main\n  File \"C:\\PySpark\\spark-3.3.2-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 593, in read_int\n    length = stream.read(4)\n  File \"C:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py\", line 705, in readinto\n    return self._sock.recv_into(b)\nTimeoutError: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\PySpark\\spark-3.3.2-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 666, in main\n  File \"C:\\PySpark\\spark-3.3.2-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 593, in read_int\n    length = stream.read(4)\n  File \"C:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py\", line 705, in readinto\n    return self._sock.recv_into(b)\nTimeoutError: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m distinct_rdd \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mcartesian(rdd)\u001b[39m.\u001b[39mfilter(\u001b[39mlambda\u001b[39;00m item: item[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m item[\u001b[39m1\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[39m# Print the distinct pairs in the resulting RDD\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m distinct_rdd\u001b[39m.\u001b[39;49mcollect()\n",
      "File \u001b[1;32mc:\\Users\\franc\\VSCode\\LargeScaleDataMining\\.venv\\lib\\site-packages\\pyspark\\rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1195\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[0;32m   1196\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1197\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[0;32m   1198\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\Users\\franc\\VSCode\\LargeScaleDataMining\\.venv\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\franc\\VSCode\\LargeScaleDataMining\\.venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 178.0 failed 1 times, most recent failure: Lost task 1.0 in stage 178.0 (TID 351) (192.168.62.78 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\PySpark\\spark-3.3.2-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 666, in main\n  File \"C:\\PySpark\\spark-3.3.2-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 593, in read_int\n    length = stream.read(4)\n  File \"C:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py\", line 705, in readinto\n    return self._sock.recv_into(b)\nTimeoutError: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\PySpark\\spark-3.3.2-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 666, in main\n  File \"C:\\PySpark\\spark-3.3.2-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 593, in read_int\n    length = stream.read(4)\n  File \"C:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py\", line 705, in readinto\n    return self._sock.recv_into(b)\nTimeoutError: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\r\n"
     ]
    }
   ],
   "source": [
    "# Use the remove_duplicate_pairs function on the RDD\n",
    "distinct_rdd = rdd.cartesian(rdd).filter(lambda item: item[0] != item[1])\n",
    "\n",
    "# Print the distinct pairs in the resulting RDD\n",
    "distinct_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda x,y: x[1] == y[0] and x[0] == y[1]\n",
    "filter(lambda item: ~any(x in item[1] for x in item[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
