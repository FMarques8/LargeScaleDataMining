{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Large Scale Data Mining assignment 1\n",
        "### Similar items\n",
        "\n",
        "Francisco Marques 97639 - Mestrado em CiÃªncia de Dados\n",
        "franciscocmarques@ua.pt\n",
        "\n",
        "\n",
        "**IMPORTANT:** please download signature matrix in .npy format from: https://github.com/FMarques8/LargeScaleDataMining/blob/aa6e20524646e6730dba4c1fbbfd9b0b3559d9b6/sig_mat.npy (about 60 MBs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3RPBVDRWmXz",
        "outputId": "77576b44-d3f4-457a-b5e0-95d6f6ee4f0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.3.2)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyspark) (0.10.9.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: sympy in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.11.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: xxhash in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: tqdm in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.65.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\franc\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: numpy in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.24.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: scipy in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scipy) (1.24.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: nltk in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2023.3.23)\n",
            "Requirement already satisfied: joblib in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: click in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\franc\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark\n",
        "%pip install sympy\n",
        "%pip install xxhash \n",
        "%pip install tqdm \n",
        "%pip install numpy\n",
        "%pip install scipy\n",
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCZJv8krMdR9",
        "outputId": "2f4a0d7c-cfaa-482d-d9e1-d0901d153732"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.3.2\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "sc = SparkContext(appName=\"Assignment1 - News Articles\")\n",
        "print(sc.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "irX1vewRO2TY",
        "outputId": "d40ecd1f-a995-4259-da4b-f570adc7b183"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://DESKTOP-PI3ARK7.Home:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Assignment1 - News Articles</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x1b725bab850>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark = SparkSession.builder.appName(\"Assignment1 - News Articles\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "P-osjw1_sVWZ"
      },
      "source": [
        "### 2.1 Tuning $b$ bands and $r$ rows \n",
        "\n",
        "Computed the number of bands and rows in order to obtain at least $90\\%$ of pairs with $85\\%$ similarity and less than $5\\%$ of pairs with $60\\%$ similarity. Although we obtained around $79\\%$ similarity, we can consider this a good value thanks to the accuracy vs complexity trade off. Reaching a similarity threshold closer to $85\\%$ would increase the computational load exponentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "05srn5UKsVc3"
      },
      "outputs": [],
      "source": [
        "def tune_params(bands, rows, sim_threshold):\n",
        "\n",
        "  sim = (1/bands)**(1/rows)\n",
        "  print(f\"We obtained a s_threshold of {sim}, should be close to {sim_threshold}, with {rows} rows and {bands} bands.\")\n",
        "\n",
        "  sim_probability = 0.85\n",
        "  p1 = sim_probability ** rows\n",
        "  print(f\"Probability of C1 and C2 identical in a given band = s^r = {p1}\")\n",
        "\n",
        "  p2 = (1 - p1) ** bands\n",
        "  print(f\"Probability C1, C2 are not similar in all of the {bands} bands = (1-s^r)^b = {p2}\")\n",
        "  if p2 < .1:\n",
        "      print(\"Condition 1 obtained\\n\")\n",
        "\n",
        "  not_sim_probability = 0.6\n",
        "  p3 = not_sim_probability ** rows\n",
        "  print(f\"Probability C1, C2 are identical in a given band = s^r = {p3}\")\n",
        "\n",
        "  p4 = 1-(1-p3) ** bands\n",
        "  print(f\"Probability C1, C2 identical in at least 1 of 20 bands: 1-(1-s^r)^b = {p4}\")\n",
        "  if p4 < 0.05:\n",
        "      print(\"Condition 2 obtained\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6E1-yQMUJTS",
        "outputId": "e2d7c548-1d7b-4d4d-d664-f11b39172d0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We obtained a s_threshold of 0.7920132050096789, should be close to 0.85, with 11 rows and 13 bands.\n",
            "Probability of C1 and C2 identical in a given band = s^r = 0.1673432436896142\n",
            "Probability C1, C2 are not similar in all of the 13 bands = (1-s^r)^b = 0.09248219617246255\n",
            "Condition 1 obtained\n",
            "\n",
            "Probability C1, C2 are identical in a given band = s^r = 0.0036279705599999985\n",
            "Probability C1, C2 identical in at least 1 of 20 bands: 1-(1-s^r)^b = 0.0461505019889491\n",
            "Condition 2 obtained\n"
          ]
        }
      ],
      "source": [
        "tune_params(13, 11, 0.85)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N_rMEcLgsV29"
      },
      "source": [
        "### 2.2 Function that, given an article, finds every article at least $85\\%$ similar.\n",
        "\n",
        "For this, we need to implement the full Locally-Sensitive Hashing (LSH) algorithm, starting with the Shingling and hashing the resulting shingles.\n",
        "\n",
        "Here we can see the structure of each JSON entry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXS8pGMFWy54",
        "outputId": "f1462522-0f7d-4fd9-f038-cb0eb3a436d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- tweet_id: string (nullable = true)\n",
            " |-- url: string (nullable = true)\n",
            "\n",
            "+--------------------+-------------------+--------------------+\n",
            "|                text|           tweet_id|                 url|\n",
            "+--------------------+-------------------+--------------------+\n",
            "|Os especialistas ...|1215246971079929857|https://www.dn.pt...|\n",
            "|Os leitores sÃ£o a...|1215233921425854465|https://publico.p...|\n",
            "|Os leitores sÃ£o a...|1223386542145724416|https://www.publi...|\n",
            "|Segundo a Associa...|1223380605897003008|https://www.cmjor...|\n",
            "|Os leitores sÃ£o a...|1223378524830097408|https://www.publi...|\n",
            "|Os leitores sÃ£o a...|1223375795999191040|https://publico.p...|\n",
            "|(Enviada diariame...|1223373056229494784|https://www.cmjor...|\n",
            "|De acordo com dad...|1223371884273766402|https://www.cmjor...|\n",
            "|                    |1223368728940818440|https://www.publi...|\n",
            "|Os leitores sÃ£o a...|1223368343710838784|https://publico.p...|\n",
            "|(Enviada diariame...|1223355638501257218|https://www.cmjor...|\n",
            "|Um cidadÃ£o italia...|1223354127184515073|https://www.jn.pt...|\n",
            "|Dono de empresa d...|1223347891051298822|https://www.cmjor...|\n",
            "|(Enviada diariame...|1223339972457127937|https://www.cmjor...|\n",
            "|Um cidadÃ£o italia...|1223335102681436164|https://www.jn.pt...|\n",
            "|Os leitores sÃ£o a...|1223332850948288514|https://publico.p...|\n",
            "|Siga-nos Jornalis...|1223326482279342086|http://expresso.p...|\n",
            "|A OrganizaÃ§Ã£o Mun...|1223325243210633218|https://www.cmjor...|\n",
            "|Os leitores sÃ£o a...|1223320219361456130|https://publico.p...|\n",
            "|Os leitores sÃ£o a...|1223319115034701825|https://www.publi...|\n",
            "+--------------------+-------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.json('data/covid_news_full.json.bz2')\n",
        "df.printSchema()\n",
        "df.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X615NyrYIhbM"
      },
      "source": [
        "#### Shingling\n",
        "\n",
        "Before we can shingle the documents, we need to clean the entries. For this, every punctuation and stopword was removed. As this algorithm can take a long time, we make use of *tqdm*'s progress bar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SYYoGWMPGNKt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm # progress bar\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from sympy import nextprime\n",
        "import json\n",
        "\n",
        "np.set_printoptions(threshold=np.inf, precision=0, suppress=True) # prevents scientific notation when printing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After cleaning the entries, we remove any entry smaller than the shingle size. Since we considered a somewhat large shingle size ($k=10$) in this notebook, entries smaller than this, were in the few cases that I looked into, always the same (\"(Enviada diariamente)\" appeared many times).\n",
        "\n",
        "Then the shingling itself, is quite simple. We made each document's shingles into a set to remove any duplicate as duplicates wont aid in finding similar items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ffnyar3sXS-4"
      },
      "outputs": [],
      "source": [
        "shingle_size = 10\n",
        "\n",
        "punctuation_table = str.maketrans(dict.fromkeys(punctuation, '')) # Table to remove punctuation from text\n",
        "stop_words = set(stopwords.words('portuguese')) # Set with stop words\n",
        "\n",
        "news_rdd = df.rdd.map(lambda item: (item['tweet_id'], item['text'])) # Removes url\n",
        "\n",
        "shingle_rdd = (news_rdd.map(lambda item: (item[0], item[1].translate(punctuation_table).split()))\n",
        "                   # Remove punctuation and stop words\n",
        "                .map(lambda item: (item[0], [w for w in item[1] if w.lower() not in stop_words]))\n",
        "                # Filter out articles with less than 'shingle_size' words\n",
        "                .filter(lambda item: len(item[1]) >= shingle_size) \n",
        "                # Shingle the documents\n",
        "                .map(lambda item: (item[0], set([tuple(item[1][i:i + shingle_size]) for i in range(len(item[1]) - shingle_size+1)])))\n",
        "                ) \n",
        "\n",
        "# number of total distinct shingles to use in the hash functions\n",
        "num_shingles = shingle_rdd.flatMap(lambda item: item[1]).distinct().count()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we define the hash function and the permutations, which are constant throughout the algorithm. We use *sympy*.nextprime(x) to find the smallest prime greater than x. The hash function is the same as in the class slides: $((a*h(x)+b)\\mod{p})\\mod{N}$ where $N$ is the number of shingles, and $p$ the next smallest prime number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tOMXbOI2pqN4"
      },
      "outputs": [],
      "source": [
        "p = nextprime(num_shingles)\n",
        "\n",
        "num_permutations = 13 * 11 # bands x rows obtained from 2.1. b*r=n; n is the number of permutations\n",
        "a = np.random.randint(1, p, size = (num_permutations))\n",
        "b = np.random.randint(0, p, size = (num_permutations))\n",
        "\n",
        "a_bc = sc.broadcast(a)\n",
        "b_bc = sc.broadcast(b)\n",
        "\n",
        "def hash_function(x, i):\n",
        "  return ((a_bc.value[i] * hash(x) + b_bc.value[i]) % p) % num_shingles"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With this we hash every shingle in each document and turn that list to a set, in order to remove duplicates. \n",
        "\n",
        "We also print some variables used throughout the algorithm such as the total number of unique shingles and number of articles. The original size is around 59 thousand, so with the previous filter we removed alot of small entries which can help speed up the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "H14KXriepxea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique shingles: 7254120\n",
            "Number of documents: 54446\n"
          ]
        }
      ],
      "source": [
        "# Hash shingles and convert list to set to remove duplicates\n",
        "hashed_shingles = shingle_rdd.map(lambda item: (item[0], set([hash(shingle) for shingle in item[1]])))\n",
        "num_docs = hashed_shingles.count()\n",
        "\n",
        "print(f'Number of unique shingles: {num_shingles}')\n",
        "print(f'Number of documents: {num_docs}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w_UNonVy0tgo"
      },
      "source": [
        "#### MinHashing\n",
        "\n",
        "Now that we have our shingles, we can perform the minhashing and create our signature matrix. This matrix will have the shape ('num_permutations', 'num_articles'), i.e., (143, 54446).\n",
        "\n",
        "For this, we use the permutation values and hash every shingle in each document, then we append the minimum value to the 'minhash_values' list, and repeat until it is done 'num_permutations' times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kaHYHIGKIoMK"
      },
      "outputs": [],
      "source": [
        "def minhash(document):\n",
        "    \"\"\"Applies minhashing to document.\"\"\"\n",
        "    minhash_values = []\n",
        "    \n",
        "    # For each permutation, get hash for every shingle and keep lowest\n",
        "    for i in range(num_permutations):\n",
        "        permuted_hash = [hash_function(shingle, i) for shingle in document]\n",
        "        minhash_values.append(min(permuted_hash))\n",
        "    \n",
        "    return minhash_values"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the speed up method of splitting the hashing into blocks, we split it into blocks of 500 entries, or 109 blocks.\n",
        "\n",
        "We also repartition the shingles RDD (hashed shingles), and cache it since it will be accesses 109 times.\n",
        "\n",
        "Then, every time a block is completed, we append its transpose (a column), to 'signature_matrix_blocks', which after every block is completed, concatenates them, creating our signature matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "K-kPi1xb0xmF",
        "outputId": "acecaa74-e635-46f4-aab8-e9380b32405b"
      },
      "outputs": [],
      "source": [
        "def signature_matrix(shingles, num_docs, block_size):\n",
        "    \"\"\"Create signature matrix using minhash.\"\"\"\n",
        "    signature_matrix = []\n",
        "    signature_matrix_blocks = []\n",
        "    \n",
        "    # Repartition RDD to lower bound (2 * cores) to reduce overhead and speed up process\n",
        "    shingles_with_ids = shingles.zipWithIndex().map(lambda x: (x[1], x[0])).repartition(32).cache()\n",
        "\n",
        "    # Process the data in blocks\n",
        "    for i in tqdm(range(0, num_docs, block_size), desc = 'Hashing blocks'):\n",
        "        # Get the next block of data\n",
        "        block = shingles_with_ids.filter(lambda x: x[0] >= i and x[0] < i + block_size).map(lambda doc: doc[1][1])\n",
        "        \n",
        "        # Compute the MinHash values for each document in the block\n",
        "        minhash_block = block.map(lambda doc: minhash(doc))\n",
        "        signature_matrix_blocks.append(np.array(minhash_block.collect()).T)\n",
        "\n",
        "    signature_matrix = np.concatenate(signature_matrix_blocks, axis = 1)\n",
        "\n",
        "    return signature_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Hashing blocks: 100%|ââââââââââ| 109/109 [59:53<00:00, 32.97s/it] \n"
          ]
        }
      ],
      "source": [
        "sig_mat = signature_matrix(hashed_shingles, num_docs, 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save signature matrix to a file\n",
        "np.save('sig_mat.npy', sig_mat)\n",
        "\n",
        "# To load it\n",
        "# sig_mat = np.load('sig_mat.npy')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LSH\n",
        "\n",
        "Finally, we can generate our candidate pairs with LSH, but first we still need to split our entries into buckets.\n",
        "\n",
        "For this we split our signature matrix into $b$ (13) bands, and in each band hash each column to one hash value, this is, we group the rows of a column in a band into a tuple, and hash that tuple. Then iterating over the hashes that were created, we split them into buckets, where the key is the hash value, this way if they have the same hash value, it means that the documents in that bucket are potential similar. Lastly, we add these buckets to a list, and repeat for the remaining bands, keeping each band's buckets *away* from the remaining. \n",
        "\n",
        "After every entry is distributed in the buckets, we go over each band's buckets and create every possible pair for buckets with at least 2 elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "IV20WfiVqp2w"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def lsh(signature_matrix, num_docs, num_bands):\n",
        "    # Split signature matrix into bands\n",
        "    bands = np.split(signature_matrix, num_bands)\n",
        "    buckets = []\n",
        "    \n",
        "    # Hash buckets by bands\n",
        "    for band in tqdm(bands, desc = 'Bucket hashing'):\n",
        "        # Hash each column r rows at a time\n",
        "        band_hashes = [hash_function(tuple(band[:, j]), 0) for j in range(num_docs)]\n",
        "        bucket_dict = {}\n",
        "        \n",
        "        # Distribute hashes over buckets\n",
        "        for j, b_hash in enumerate(band_hashes):\n",
        "            if b_hash not in bucket_dict:\n",
        "                bucket_dict[b_hash] = []\n",
        "            bucket_dict[b_hash].append(j)\n",
        "        buckets.append(bucket_dict)\n",
        "    \n",
        "    # Generate candidate pairs if they are in the same bucket\n",
        "    candidate_pairs = set()\n",
        "    for bucket in tqdm(buckets, desc = 'Candidate pairs generation'):\n",
        "        for b_hash, docs in list(bucket.items()):\n",
        "            if len(docs) < 2:\n",
        "                continue\n",
        "            for pair in combinations(docs, 2):\n",
        "                candidate_pairs.add(pair)\n",
        "\n",
        "    return list(candidate_pairs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we call the function and print the total number of candidates, and we can see that we have many candidates, which we will confirm if they are false positives or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iquAgD9UeV2M",
        "outputId": "dc970520-b943-4d64-e705-a1c0a3e6500c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Bucket hashing: 100%|ââââââââââ| 13/13 [00:10<00:00,  1.26it/s]\n",
            "Candidate pairs generation: 100%|ââââââââââ| 13/13 [00:04<00:00,  2.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total candidate pairs found: 229154\n"
          ]
        }
      ],
      "source": [
        "candidate_pairs = lsh(sig_mat, num_docs, 13)\n",
        "\n",
        "print(f'Total candidate pairs found: {len(candidate_pairs)}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we create out *jaccard* similarity function, and the function to find similar items in the candidate pairs. Since this function can be considered brute force since we're making sure if they are similar or not, it will take much longer than the previous algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def jaccard(set1: set[int], set2: set[int]):\n",
        "    \"\"\"Jaccard similarity between set1 and set2.\"\"\"\n",
        "    return len(set1.intersection(set2)) / len(set1.union(set2))\n",
        "\n",
        "def get_similar(id, candidate_pairs, threshold, idx_to_id, hashed_rdd):\n",
        "    \"\"\"Find similar articles to the input based on the Jaccard similarity.\"\"\"\n",
        "    # Unpack pairs \n",
        "    candidate_documents = set([x for pair in candidate_pairs for x in pair])\n",
        "    candidates_rdd = hashed_rdd.filter(lambda item: item[1] in candidate_documents).collect() # Collect 'real' candidates\n",
        "    \n",
        "    document_dict = {key: x for x, key in candidates_rdd} # Dict of document data -> {index: (id, shingles)}\n",
        "    similar_articles = []\n",
        "\n",
        "    for pair in candidate_pairs:\n",
        "        if id in pair:\n",
        "            sim = jaccard(document_dict[pair[0]][1], document_dict[pair[1]][1])\n",
        "            if sim >= threshold:\n",
        "                suggestion_id = set(pair) - set((id,))\n",
        "                similar_articles.append(idx_to_id[suggestion_id.pop()])\n",
        "    return similar_articles"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Performance Evaluation\n",
        "Now we must test the performance of our implementation, by running for several random samples and obtain false positives and false negatives. With this first cell we can verify if the candidates really are similar and compute the number of false positives. The approach for the false negatives is a bit different since we need to create every pair with a given article, and then compute their similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|ââââââââââ| 200/200 [54:17<00:00, 16.29s/it]\n"
          ]
        }
      ],
      "source": [
        "# Dictionary with articles index and id\n",
        "idx_to_id = {key: id[0] for id, key in hashed_shingles.zipWithIndex().collect()}\n",
        "cached_hashed_shingles = hashed_shingles.zipWithIndex().repartition(32).cache()\n",
        "\n",
        "# Unpack random 200 pairs\n",
        "random_candidates = set([candidate_pairs[idx][0] for idx in np.random.choice(len(candidate_pairs), 200)])\n",
        "\n",
        "# Make sure there are 200 candidates\n",
        "while len(random_candidates) < 200:\n",
        "    random_candidates.add(candidate_pairs[np.random.randint(0, len(candidate_pairs), 1)[0]][0])\n",
        "\n",
        "suggestions = {idx_to_id[id]: [] for id in random_candidates}\n",
        "\n",
        "# Verify how many candidate pairs are similar, save to dictionary regardless of being similar or not\n",
        "for candidate in tqdm(random_candidates):\n",
        "    similar_articles = get_similar(candidate, candidate_pairs, 0.85, idx_to_id, cached_hashed_shingles)\n",
        "    candidate_id = idx_to_id[candidate] # Convert the index to the article id\n",
        "    suggestions[candidate_id].extend(similar_articles)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the dictionary to a JSON file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('suggestions.json', 'w') as f:\n",
        "    json.dump(suggestions, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load with\n",
        "with open('suggestions.json', 'r') as f:\n",
        "    suggestions = json.load(f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "False negatives, this takes a long time to compute as it iterates over the whole data set. This will take up a lot of memory since it will load the entire hashed shingles RDD to memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{29184: [], 14340: [], 31749: [], 3078: [], 13321: [], 11788: [], 33298: [], 29202: ['1241289993315893248'], 9239: [], 46105: [], 8731: [], 14878: [], 16416: [], 8742: [], 23085: [], 12849: [], 3122: [], 19507: [], 2046: [], 50232: [], 2112: [], 48708: [], 6212: [], 21574: [], 4167: [], 12868: [], 28742: [], 42569: [], 13388: [], 50258: [], 3154: [], 19028: [], 32341: [], 21081: [], 1116: [], 25701: [], 16487: [], 33896: [], 4199: ['1242185523147485187'], 4207: [], 5231: [], 4212: [], 9335: [], 43640: [], 42105: [], 7290: [], 53247: [], 50809: [], 19581: [], 5760: [], 21639: [], 38025: [], 2186: [], 9355: [], 36494: [], 21135: [], 156: [], 1181: [], 43166: [], 37025: [], 32933: [], 34981: [], 1191: [], 15013: [], 2725: [], 12458: [], 13997: [], 22704: [], 12464: [], 39096: [], 12990: [], 46270: [], 1216: [], 3777: [], 49346: [], 17598: [], 26313: [], 40137: [], 18644: [], 9434: [], 42205: [], 19677: [], 8415: [], 48865: [], 33507: [], 6376: [], 237: [], 40174: [], 1776: [], 35568: [], 9458: [], 3829: [], 22265: [], 23294: [], 6403: [], 7949: [], 40718: [], 2840: [], 6425: [], 5401: [], 2846: [], 288: [], 3873: [], 7972: [], 4906: [], 13612: [], 53037: [], 23854: [], 44849: [], 4914: [], 21814: [], 43832: [], 34107: [], 4924: [], 46397: [], 10048: [], 9025: [], 42308: [], 3908: [], 33094: [], 17227: [], 13644: [], 44368: [], 9555: [], 10584: [], 41818: [], 17761: [], 18788: [], 3430: [], 24425: [], 3437: [], 37741: [], 2415: [], 4465: [], 40818: [], 8051: [], 6005: [], 18806: [], 23928: [], 35707: [], 33660: [], 4990: [], 11647: [], 44926: [], 10115: [], 18307: [], 1413: [], 15749: [], 38276: [], 50060: [], 36240: [], 21905: [], 48016: [], 46993: [], 38800: [], 41365: [], 1433: [], 17307: [], 13211: [], 34207: [], 10145: [], 16293: [], 15781: [], 43942: [], 20394: [], 15788: [], 25016: [], 34232: [], 2488: [], 33723: [], 9662: [], 23998: [], 43969: [], 9154: [], 2500: [], 37318: [], 2502: [], 16329: [], 47051: [], 32206: [], 11223: [], 3544: [], 11226: [], 11227: [], 37340: [], 29660: [], 1504: [], 11236: [], 22502: [], 11239: [], 32232: [], 32240: [], 52209: [], 3570: [], 1010: [], 2037: ['1239334039871504394'], 3064: [], 44026: [], 44030: [], 2047: ['1239372711312404487']}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|ââââââââââ| 200/200 [06:15<00:00,  1.88s/it]\n"
          ]
        }
      ],
      "source": [
        "# Choose 200 random ids to test the function\n",
        "random_ids = set([idx for idx in np.random.choice(len(candidate_pairs), 100)])\n",
        "\n",
        "# Make sure there are 100 candidates\n",
        "while len(random_candidates) < 100:\n",
        "    random_candidates.add(np.random.randint(0, len(candidate_pairs), 1))\n",
        "\n",
        "# If loading JSON, need to add way to reverse back to index\n",
        "random_keys = suggestions\n",
        "\n",
        "cached_hashed_shingles = hashed_shingles.zipWithIndex().repartition(32).cache()\n",
        "idx_to_id = {key: id for id, key in cached_hashed_shingles.collect()} # Index to document id \n",
        "\n",
        "id_to_idx = {key[0]: id for id, key in idx_to_id.items()} # convert back to index\n",
        "\n",
        "random_ids = {id_to_idx[id]: vals for id, vals in random_keys.items()}\n",
        "\n",
        "# Dictionary with similarities\n",
        "sim_dict = {id: {'similar': [], 'not similar': []} for id in random_ids} \n",
        "\n",
        "for id in tqdm(random_ids):\n",
        "    possible_pairs = set([(id, idx) for idx in range(num_docs) if id != idx])\n",
        "\n",
        "    for pair in possible_pairs:\n",
        "        sim = jaccard(idx_to_id[pair[0]][1], idx_to_id[pair[1]][1])\n",
        "        if sim >= 0.85:\n",
        "            sim_dict[id]['similar'].append(sim)\n",
        "        else:\n",
        "            sim_dict[id]['not similar'].append(sim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|ââââââââââ| 200/200 [04:30<00:00,  1.35s/it]\n"
          ]
        }
      ],
      "source": [
        "performance = {id: {'FP': 0, 'FN': 0} for id in random_ids.keys()}\n",
        "\n",
        "for id in tqdm(random_ids):\n",
        "    for pair in candidate_pairs:\n",
        "        if id in pair:\n",
        "            if pair[0] in sim_dict[id]['not similar'] or pair[1] in sim_dict[id]['not similar']:\n",
        "                performance[id]['FP'] += 1\n",
        "        performance[id]['FN'] = len(sim_dict[id]['similar']) - performance[id]['FP']\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can finally take a look at the performance, and we can see that there are many False Negatives, which means that the LSH method did not have the best performance, one of the ways we could get better results would be to get closer to the 85% similarity by changing the number of bands and rows, since they were tuned for about 79%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1318288932296065024 {'FP': 0, 'FN': 0}\n",
            "1248917588308000768 {'FP': 0, 'FN': 0}\n",
            "1329709945248419841 {'FP': 0, 'FN': 0}\n",
            "1241421869590491136 {'FP': 0, 'FN': 2}\n",
            "1255795486415609856 {'FP': 0, 'FN': 1}\n",
            "1245413319336435712 {'FP': 0, 'FN': 1}\n",
            "1325318690691293184 {'FP': 0, 'FN': 1}\n",
            "1318244017772036097 {'FP': 0, 'FN': 158}\n",
            "1246349622861275136 {'FP': 0, 'FN': 1}\n",
            "1388625937651179520 {'FP': 0, 'FN': 1}\n",
            "1251780613151096834 {'FP': 0, 'FN': 0}\n",
            "1265983033263435778 {'FP': 0, 'FN': 0}\n",
            "1258364289766785024 {'FP': 0, 'FN': 0}\n",
            "1251773860032151553 {'FP': 0, 'FN': 0}\n",
            "1282019664185491458 {'FP': 0, 'FN': 2}\n",
            "1247185742859730947 {'FP': 0, 'FN': 6}\n",
            "1241390069774266369 {'FP': 0, 'FN': 0}\n",
            "1275726780800925699 {'FP': 0, 'FN': 0}\n",
            "1239335744092725249 {'FP': 0, 'FN': 1}\n",
            "1422217217219964932 {'FP': 0, 'FN': 0}\n",
            "1239262050809008132 {'FP': 0, 'FN': 1}\n",
            "1410516950409781248 {'FP': 0, 'FN': 0}\n",
            "1242818687633182720 {'FP': 0, 'FN': 0}\n",
            "1268529078518366208 {'FP': 0, 'FN': 1}\n",
            "1240393283047763974 {'FP': 0, 'FN': 202}\n",
            "1250194740844400641 {'FP': 0, 'FN': 2}\n",
            "1319986275789701121 {'FP': 0, 'FN': 0}\n",
            "1368889637352136704 {'FP': 0, 'FN': 2}\n",
            "1255619290247634945 {'FP': 0, 'FN': 0}\n",
            "1421862126667370498 {'FP': 0, 'FN': 1}\n",
            "1241357249618104321 {'FP': 0, 'FN': 0}\n",
            "1259480570750435328 {'FP': 0, 'FN': 0}\n",
            "1330081735443435521 {'FP': 0, 'FN': 0}\n",
            "1276465623359422464 {'FP': 0, 'FN': 0}\n",
            "1229569084775444484 {'FP': 0, 'FN': 0}\n",
            "1293209479715184647 {'FP': 0, 'FN': 0}\n",
            "1258153840420978694 {'FP': 0, 'FN': 167}\n",
            "1326230822865788930 {'FP': 0, 'FN': 0}\n",
            "1242269456702869506 {'FP': 0, 'FN': 1}\n",
            "1242249444080435200 {'FP': 0, 'FN': 1}\n",
            "1237423922250907648 {'FP': 0, 'FN': 1}\n",
            "1242244899216515074 {'FP': 0, 'FN': 0}\n",
            "1246132057409019910 {'FP': 0, 'FN': 0}\n",
            "1376776521923375104 {'FP': 0, 'FN': 2}\n",
            "1371397574205267973 {'FP': 0, 'FN': 1}\n",
            "1240938909867130880 {'FP': 0, 'FN': 1}\n",
            "1474851390073843716 {'FP': 0, 'FN': 0}\n",
            "1438263192199778307 {'FP': 0, 'FN': 0}\n",
            "1275434507370283008 {'FP': 0, 'FN': 0}\n",
            "1234516111770767367 {'FP': 0, 'FN': 0}\n",
            "1268232818506825728 {'FP': 0, 'FN': 0}\n",
            "1347112113320452096 {'FP': 0, 'FN': 1}\n",
            "1244004879149993984 {'FP': 0, 'FN': 1}\n",
            "1246119961434734592 {'FP': 0, 'FN': 3}\n",
            "1339316986141093895 {'FP': 0, 'FN': 0}\n",
            "1276230303053463554 {'FP': 0, 'FN': 1}\n",
            "1222182411959783424 {'FP': 0, 'FN': 0}\n",
            "1228625605597552641 {'FP': 0, 'FN': 0}\n",
            "1366747777196318726 {'FP': 0, 'FN': 1}\n",
            "1336444745401380864 {'FP': 0, 'FN': 0}\n",
            "1327009676828008449 {'FP': 0, 'FN': 1}\n",
            "1337403096692035585 {'FP': 0, 'FN': 111}\n",
            "1232788512539320320 {'FP': 0, 'FN': 1}\n",
            "1264213329565941760 {'FP': 0, 'FN': 0}\n",
            "1244587970511937537 {'FP': 0, 'FN': 0}\n",
            "1246021042591670272 {'FP': 0, 'FN': 0}\n",
            "1249666734644236289 {'FP': 0, 'FN': 0}\n",
            "1286346879622680576 {'FP': 0, 'FN': 0}\n",
            "1246016144579923970 {'FP': 0, 'FN': 1}\n",
            "1351482726323531776 {'FP': 0, 'FN': 0}\n",
            "1250067073381867520 {'FP': 0, 'FN': 0}\n",
            "1398225961829736451 {'FP': 0, 'FN': 0}\n",
            "1232664768806256641 {'FP': 0, 'FN': 0}\n",
            "1239889566775021569 {'FP': 0, 'FN': 0}\n",
            "1426299699003281409 {'FP': 0, 'FN': 0}\n",
            "1265576929895354369 {'FP': 0, 'FN': 0}\n",
            "1296008025484951552 {'FP': 0, 'FN': 1}\n",
            "1361436305893314560 {'FP': 0, 'FN': 0}\n",
            "1261982641571708928 {'FP': 0, 'FN': 0}\n",
            "1251491406054653960 {'FP': 0, 'FN': 0}\n",
            "1370466043295113217 {'FP': 0, 'FN': 0}\n",
            "1275136967148175361 {'FP': 0, 'FN': 1}\n",
            "1252312765894742021 {'FP': 0, 'FN': 0}\n",
            "1419320133126959109 {'FP': 0, 'FN': 1}\n",
            "1323988540259270657 {'FP': 0, 'FN': 0}\n",
            "1242580097187696640 {'FP': 0, 'FN': 2}\n",
            "1221103818685014022 {'FP': 0, 'FN': 0}\n",
            "1361381179237236739 {'FP': 0, 'FN': 0}\n",
            "1239629275902476288 {'FP': 0, 'FN': 17}\n",
            "1344579216927584256 {'FP': 0, 'FN': 0}\n",
            "1251462086909145089 {'FP': 0, 'FN': 0}\n",
            "1239835394214166528 {'FP': 0, 'FN': 1}\n",
            "1284203450260246533 {'FP': 0, 'FN': 0}\n",
            "1281216679704494086 {'FP': 0, 'FN': 1}\n",
            "1242548933022437377 {'FP': 0, 'FN': 1}\n",
            "1239155794001760256 {'FP': 0, 'FN': 1}\n",
            "1357293457820295169 {'FP': 0, 'FN': 0}\n",
            "1244357581302312962 {'FP': 0, 'FN': 0}\n",
            "1242535567272947712 {'FP': 0, 'FN': 0}\n",
            "1237078415150321671 {'FP': 0, 'FN': 0}\n",
            "1244346258887782401 {'FP': 0, 'FN': 0}\n",
            "1220289002915409927 {'FP': 0, 'FN': 0}\n",
            "1240785209072771073 {'FP': 0, 'FN': 0}\n",
            "1239087857299591168 {'FP': 0, 'FN': 0}\n",
            "1238056816132710405 {'FP': 0, 'FN': 1}\n",
            "1255243019894378504 {'FP': 0, 'FN': 0}\n",
            "1460308788506992640 {'FP': 0, 'FN': 0}\n",
            "1279033736261242880 {'FP': 0, 'FN': 0}\n",
            "1388119142713204739 {'FP': 0, 'FN': 1}\n",
            "1238046412102873089 {'FP': 0, 'FN': 0}\n",
            "1267597382457442313 {'FP': 0, 'FN': 0}\n",
            "1381920724500238338 {'FP': 0, 'FN': 1}\n",
            "1323918190553059329 {'FP': 0, 'FN': 0}\n",
            "1238031473892098054 {'FP': 0, 'FN': 0}\n",
            "1397292469495205890 {'FP': 0, 'FN': 2}\n",
            "1254398130176614402 {'FP': 0, 'FN': 0}\n",
            "1246754986488406016 {'FP': 0, 'FN': 3}\n",
            "1370317564224667649 {'FP': 0, 'FN': 0}\n",
            "1240737664493596680 {'FP': 0, 'FN': 1}\n",
            "1326420796584112130 {'FP': 0, 'FN': 0}\n",
            "1267074912889573377 {'FP': 0, 'FN': 2}\n",
            "1255192203066048512 {'FP': 0, 'FN': 0}\n",
            "1379016130648535043 {'FP': 0, 'FN': 0}\n",
            "1251214102573338624 {'FP': 0, 'FN': 0}\n",
            "1253315356694736897 {'FP': 0, 'FN': 0}\n",
            "1364560258874621952 {'FP': 0, 'FN': 0}\n",
            "1265043278669496320 {'FP': 0, 'FN': 167}\n",
            "1261535903861874688 {'FP': 0, 'FN': 0}\n",
            "1240274325674041347 {'FP': 0, 'FN': 22}\n",
            "1283674159072129025 {'FP': 0, 'FN': 1}\n",
            "1240269884208136193 {'FP': 0, 'FN': 1}\n",
            "1344998228245274626 {'FP': 0, 'FN': 0}\n",
            "1243625922030338049 {'FP': 0, 'FN': 0}\n",
            "1242048656762638336 {'FP': 0, 'FN': 0}\n",
            "1356226147252662274 {'FP': 0, 'FN': 1}\n",
            "1252942259592613888 {'FP': 0, 'FN': 1}\n",
            "1244998373566042112 {'FP': 0, 'FN': 0}\n",
            "1260108183022034945 {'FP': 0, 'FN': 0}\n",
            "1278685061521514496 {'FP': 0, 'FN': 0}\n",
            "1343852297500155906 {'FP': 0, 'FN': 1}\n",
            "1322963033593884680 {'FP': 0, 'FN': 0}\n",
            "1237850687309934594 {'FP': 0, 'FN': 2}\n",
            "1245667841401110528 {'FP': 0, 'FN': 1}\n",
            "1388192133576802305 {'FP': 0, 'FN': 1}\n",
            "1254207573538078721 {'FP': 0, 'FN': 1}\n",
            "1260489283762348032 {'FP': 0, 'FN': 0}\n",
            "1242879163805163526 {'FP': 0, 'FN': 0}\n",
            "1263531756348678146 {'FP': 0, 'FN': 1}\n",
            "1353139798702252032 {'FP': 0, 'FN': 2}\n",
            "1426993573745119237 {'FP': 0, 'FN': 0}\n",
            "1340456425814896641 {'FP': 0, 'FN': 0}\n",
            "1275901782905012224 {'FP': 0, 'FN': 0}\n",
            "1416063942808510465 {'FP': 0, 'FN': 0}\n",
            "1407249462754488322 {'FP': 0, 'FN': 0}\n",
            "1354485747634630664 {'FP': 0, 'FN': 0}\n",
            "1358817332253110272 {'FP': 0, 'FN': 1}\n",
            "1242866533434564608 {'FP': 0, 'FN': 202}\n",
            "1266729345684508675 {'FP': 0, 'FN': 0}\n",
            "1255915272122310661 {'FP': 0, 'FN': 0}\n",
            "1322946844683587588 {'FP': 0, 'FN': 1}\n",
            "1254118093896499200 {'FP': 0, 'FN': 0}\n",
            "1258633034917654537 {'FP': 0, 'FN': 0}\n",
            "1263483607244509189 {'FP': 0, 'FN': 0}\n",
            "1381625662839083012 {'FP': 0, 'FN': 0}\n",
            "1271956209361465346 {'FP': 0, 'FN': 0}\n",
            "1263457030242738178 {'FP': 0, 'FN': 1}\n",
            "1298272782086627328 {'FP': 0, 'FN': 0}\n",
            "1333452230851170304 {'FP': 0, 'FN': 0}\n",
            "1243574051496497152 {'FP': 0, 'FN': 1}\n",
            "1333441007208308742 {'FP': 0, 'FN': 0}\n",
            "1251127205398302720 {'FP': 0, 'FN': 0}\n",
            "1278396886832078857 {'FP': 0, 'FN': 0}\n",
            "1381579887245410304 {'FP': 0, 'FN': 0}\n",
            "1246463960653627394 {'FP': 0, 'FN': 0}\n",
            "1243567709721370625 {'FP': 0, 'FN': 0}\n",
            "1349705595037876230 {'FP': 0, 'FN': 0}\n",
            "1243567398650863618 {'FP': 0, 'FN': 0}\n",
            "1258474107907321857 {'FP': 0, 'FN': 0}\n",
            "1406957780808908804 {'FP': 0, 'FN': 0}\n",
            "1332351088780042242 {'FP': 0, 'FN': 0}\n",
            "1247569805269442560 {'FP': 0, 'FN': 1}\n",
            "1240165082325299200 {'FP': 0, 'FN': 1}\n",
            "1247566868287827968 {'FP': 0, 'FN': 0}\n",
            "1247566223556124672 {'FP': 0, 'FN': 0}\n",
            "1349462645842259968 {'FP': 0, 'FN': 0}\n",
            "1316425788971986947 {'FP': 0, 'FN': 158}\n",
            "1243512538966437888 {'FP': 0, 'FN': 1}\n",
            "1247557775414177793 {'FP': 0, 'FN': 0}\n",
            "1287703918147973122 {'FP': 0, 'FN': 3}\n",
            "1247556981021343746 {'FP': 0, 'FN': 0}\n",
            "1332302068032425989 {'FP': 0, 'FN': 0}\n",
            "1332284643526250498 {'FP': 0, 'FN': 0}\n",
            "1455631724176625675 {'FP': 0, 'FN': 0}\n",
            "1240072986348130305 {'FP': 0, 'FN': 3}\n",
            "1233071987255390208 {'FP': 0, 'FN': 0}\n",
            "1239372711312404487 {'FP': 0, 'FN': 3}\n",
            "1241434898986143749 {'FP': 0, 'FN': 0}\n",
            "1380636015518486531 {'FP': 0, 'FN': 0}\n",
            "1380537404789026816 {'FP': 0, 'FN': 0}\n",
            "1239334039871504394 {'FP': 0, 'FN': 3}\n"
          ]
        }
      ],
      "source": [
        "for idx, metrics in performance.items():\n",
        "    print(idx_to_id[idx][0], metrics)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
